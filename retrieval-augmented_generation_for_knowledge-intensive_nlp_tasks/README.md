# Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks - ë…¼ë¬¸ ë¦¬ë·°

## ğŸ“Œ ë…¼ë¬¸ ì†Œê°œ

**ì œëª©**: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks
**ì €ì**: Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel, Sebastian Riedel, Douwe Kiela
**ì†Œì†**: Facebook AI Research, University College London, New York University
**í•™íšŒ**: NeurIPS 2020
**arXiv**: https://arxiv.org/abs/2005.11401

## ğŸ¯ í•µì‹¬ ê°€ì¹˜

ì´ ë…¼ë¬¸ì€ **í˜„ëŒ€ LLM ì‹œìŠ¤í…œì˜ ì‚¬ì‹¤ìƒ í‘œì¤€ ì•„í‚¤í…ì²˜**ê°€ ëœ RAG(Retrieval-Augmented Generation)ë¥¼ ì œì•ˆí•œ íšê¸°ì ì¸ ì—°êµ¬ì…ë‹ˆë‹¤.

### ì™œ í˜ì‹ ì ì¸ê°€?

**1. Hallucination ë¬¸ì œ í•´ê²°**
- LLMì´ ì‚¬ì‹¤ì´ ì•„ë‹Œ ë‚´ìš©ì„ ê·¸ëŸ´ë“¯í•˜ê²Œ ìƒì„±í•˜ëŠ” ë¬¸ì œ
- ì™¸ë¶€ ì§€ì‹ ê²€ìƒ‰ì„ í†µí•´ ê·¼ê±° ìˆëŠ” ë‹µë³€ ìƒì„±

**2. ì§€ì‹ ì—…ë°ì´íŠ¸ ê°€ëŠ¥**
- ê¸°ì¡´: ìƒˆë¡œìš´ ì§€ì‹ â†’ ëª¨ë¸ ì „ì²´ ì¬í•™ìŠµ í•„ìš” (ìˆ˜ë°±ë§Œ ë‹¬ëŸ¬ ë¹„ìš©)
- RAG: ë¬¸ì„œ ì¸ë±ìŠ¤ë§Œ êµì²´ â†’ ì¦‰ì‹œ ìµœì‹  ì •ë³´ ë°˜ì˜

**3. íˆ¬ëª…ì„±ê³¼ ì‹ ë¢°ì„±**
- ìƒì„±ëœ ë‹µë³€ì˜ ì¶œì²˜ë¥¼ ëª…í™•íˆ ì¶”ì  ê°€ëŠ¥
- "ì™œ ì´ëŸ° ë‹µë³€ì„ í–ˆëŠ”ê°€?"ì— ëŒ€í•œ ì„¤ëª… ì œê³µ

**4. íŒŒë¼ë¯¸í„° íš¨ìœ¨ì„±**
- **626M** íŒŒë¼ë¯¸í„° RAG > **11B** íŒŒë¼ë¯¸í„° T5
- ì‘ì€ ëª¨ë¸ë¡œë„ ë” ë‚˜ì€ ì„±ëŠ¥ ë‹¬ì„±

### ì‹¤ë¬´ì—ì„œì˜ ì˜í–¥

- **ChatGPT Enterprise, Claude for Work**: RAG ê¸°ë°˜ ì§€ì‹ ê²€ìƒ‰
- **Microsoft Copilot**: ë¬¸ì„œ ê²€ìƒ‰ í†µí•©
- **ê¸°ì—…ìš© Q&A ì‹œìŠ¤í…œ**: ì‚¬ë‚´ ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ
- **ë²•ë¥ /ì˜ë£Œ AI**: ì •í™•í•œ ì¶œì²˜ê°€ ì¤‘ìš”í•œ ë¶„ì•¼
- **RAG ìƒíƒœê³„**: LangChain, LlamaIndex, Haystack ë“±

## ğŸ”™ ì—°êµ¬ ë°°ê²½ ë° ë™ê¸°

### ê¸°ì¡´ ì ‘ê·¼ë²•ì˜ í•œê³„

**1. Closed-Book QA (Parametric-only Models)**

ì „í˜•ì ì¸ ì˜ˆ: T5, GPT-3 ë“± ëŒ€í˜• ì–¸ì–´ëª¨ë¸

```
User: "ë°”ë½ ì˜¤ë°”ë§ˆëŠ” ì–´ë””ì„œ íƒœì–´ë‚¬ë‚˜ìš”?"
T5-11B: "í•˜ì™€ì´ í˜¸ë†€ë£°ë£¨" (íŒŒë¼ë¯¸í„°ì— ì €ì¥ëœ ì§€ì‹)
```

**ë¬¸ì œì **:
- **Hallucination**: í•™ìŠµ ë°ì´í„°ì— ì—†ëŠ” ë‚´ìš©ì„ ê·¸ëŸ´ë“¯í•˜ê²Œ ìƒì„±
- **ì§€ì‹ ê³ ì •**: í•™ìŠµ ì‹œì  ì´í›„ ì •ë³´ëŠ” ëª¨ë¦„ (GPT-4ë„ 2023ë…„ 4ì›”ê¹Œì§€ë§Œ)
- **í¸í–¥**: í•™ìŠµ ë°ì´í„°ì˜ í¸í–¥ì„ ê·¸ëŒ€ë¡œ ë°˜ì˜
- **ë¹„ìš©**: ì§€ì‹ ì—…ë°ì´íŠ¸ = ì „ì²´ ì¬í•™ìŠµ (ìˆ˜ì²œë§Œ~ìˆ˜ì–µ ë‹¬ëŸ¬)
- **ë¶ˆíˆ¬ëª…ì„±**: ì–´ë–»ê²Œ ê·¸ ë‹µì„ ì•Œì•˜ëŠ”ì§€ ì„¤ëª… ë¶ˆê°€

**2. Open-Book QA (Retrieval-only Models)**

ì „í˜•ì ì¸ ì˜ˆ: DPR, BM25 + Extractive QA

```
User: "ë°”ë½ ì˜¤ë°”ë§ˆëŠ” ì–´ë””ì„œ íƒœì–´ë‚¬ë‚˜ìš”?"
System:
  1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰: "Barack Obama (born August 4, 1961) is an American..."
  2. Span ì¶”ì¶œ: "Honolulu, Hawaii"
```

**ë¬¸ì œì **:
- **ì¶”ì¶œë§Œ ê°€ëŠ¥**: ë¬¸ì„œì— ì •í™•íˆ ë‚˜ì˜¨ í…ìŠ¤íŠ¸ë§Œ ë‹µë³€ ê°€ëŠ¥
- **ìì—°ìŠ¤ëŸ¬ìš´ ìƒì„± ë¶ˆê°€**: "ê·¸ëŠ” 1961ë…„ í•˜ì™€ì´ì—ì„œ íƒœì–´ë‚¬ìŠµë‹ˆë‹¤" ê°™ì€ ë¬¸ì¥ ìƒì„± ëª»í•¨
- **ë‹¤ì¤‘ ë¬¸ì„œ ì¡°í•© ì–´ë ¤ì›€**: ì—¬ëŸ¬ ë¬¸ì„œì˜ ì •ë³´ë¥¼ ì¢…í•©í•œ ë‹µë³€ ìƒì„± ë¶ˆê°€
- **ì¼ë°˜í™” ë¶€ì¡±**: QA ì™¸ ë‹¤ë¥¸ íƒœìŠ¤í¬(ìš”ì•½, ëŒ€í™”)ì— ì ìš© ì–´ë ¤ì›€

### ì´ ë…¼ë¬¸ì´ í•´ê²°í•˜ê³ ì í•œ í•µì‹¬ ê³¼ì œ

1. **Parametric + Non-Parametric ê²°í•©**: LLMì˜ ì–¸ì–´ ì´í•´ë ¥ + ì™¸ë¶€ ì§€ì‹ì˜ ì •í™•ì„±
2. **ìœ ì—°í•œ ìƒì„±**: ê²€ìƒ‰ëœ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ì§€ ì•Šê³  ìì—°ìŠ¤ëŸ½ê²Œ ì¬êµ¬ì„±
3. **End-to-End í•™ìŠµ**: Retrieverì™€ Generatorë¥¼ ê³µë™ í•™ìŠµ
4. **ë²”ìš©ì„±**: Open-QAë¿ ì•„ë‹ˆë¼ ìƒì„±, ê²€ì¦ ë“± ë‹¤ì–‘í•œ íƒœìŠ¤í¬ì— ì ìš©

## ğŸ—ï¸ ëª¨ë¸ ì•„í‚¤í…ì²˜

### ì „ì²´ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      RAG Pipeline                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Input Query (x)
    "Who wrote Python?"
         â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Retriever    â”‚  â† DPR (Dense Passage Retrieval)
    â”‚  BERT-base     â”‚     Query Encoder + Document Encoder
    â”‚  (110M params) â”‚     FAISS Index (21M documents)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    Top-K Documents (zâ‚, zâ‚‚, ..., zâ‚–)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ zâ‚: "Python was created by Guido van Rossum..."  â”‚
    â”‚ zâ‚‚: "Guido van Rossum began working on..."       â”‚
    â”‚ zâ‚ƒ: "The programming language Python..."         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Generator    â”‚  â† BART-large
    â”‚   BART-large   â”‚     Seq2Seq Transformer
    â”‚  (400M params) â”‚     Input: concat(query, doc)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    Generated Answer (y)
    "Python was created by Guido van Rossum in 1991."
         â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Marginalizationâ”‚  â† RAG-Sequence or RAG-Token
    â”‚  Over Top-K    â”‚     p(y|x) = Î£ p(z|x) Ã— p(y|x,z)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1. Retriever: DPR (Dense Passage Retrieval)

**ì—­í• **: ì¿¼ë¦¬ì™€ ì˜ë¯¸ì ìœ¼ë¡œ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ë¹ ë¥´ê²Œ ê²€ìƒ‰

**êµ¬ì¡°**: Bi-Encoder ì•„í‚¤í…ì²˜

```python
# Query Encoder
q(x) = BERT_query(x)  # [CLS] í† í°ì˜ 768-dim ë²¡í„°

# Document Encoder
d(z) = BERT_doc(z)    # [CLS] í† í°ì˜ 768-dim ë²¡í„°

# ê²€ìƒ‰ í™•ë¥  (ë‚´ì  ê¸°ë°˜)
p_Î·(z|x) âˆ exp(d(z)áµ€ q(x))
```

**ì™œ Bi-Encoderì¸ê°€?**

| êµ¬ë¶„ | Bi-Encoder | Cross-Encoder |
|------|-----------|---------------|
| êµ¬ì¡° | Query, Doc ë…ë¦½ ì¸ì½”ë”© | Query+Doc í•¨ê»˜ ì¸ì½”ë”© |
| ì‚¬ì „ ê³„ì‚° | âœ… ë¬¸ì„œ ë²¡í„° ë¯¸ë¦¬ ê³„ì‚° ê°€ëŠ¥ | âŒ ì¿¼ë¦¬ë§ˆë‹¤ ëª¨ë“  ë¬¸ì„œ ì¬ê³„ì‚° |
| ê²€ìƒ‰ ì†ë„ | âš¡ Sub-linear (FAISS) | ğŸŒ Linear scan í•„ìš” |
| ì •í™•ë„ | ë‚®ìŒ (ìƒí˜¸ì‘ìš© ì—†ìŒ) | ë†’ìŒ (full attention) |
| ì í•©ì„± | 1ì°¨ ê²€ìƒ‰ (Top-K ì¶”ì¶œ) | 2ì°¨ Re-ranking |

**ê²€ìƒ‰ ê³¼ì •**:

```python
# 1. Offline: ëª¨ë“  ë¬¸ì„œ ë²¡í„°í™” (1íšŒë§Œ ìˆ˜í–‰)
doc_vectors = []
for doc in all_documents:
    doc_vectors.append(BERT_doc(doc))  # 21M documents â†’ 21M vectors

# FAISS ì¸ë±ìŠ¤ êµ¬ì¶•
import faiss
index = faiss.IndexFlatIP(768)  # Inner Product
index.add(doc_vectors)

# 2. Online: ì¿¼ë¦¬ ì‹œ ë¹ ë¥¸ ê²€ìƒ‰
query_vector = BERT_query("Who wrote Python?")
scores, doc_ids = index.search(query_vector, k=5)  # Top-5 ê²€ìƒ‰

# ì‹œê°„ ë³µì¡ë„: O(log N) with HNSW index
```

**FAISS (Facebook AI Similarity Search)**:
- **ëª©ì **: ìˆ˜ì–µ ê°œ ë²¡í„°ì—ì„œ ìµœê·¼ì ‘ ì´ì›ƒì„ ë¹ ë¥´ê²Œ ê²€ìƒ‰
- **ë°©ë²•**: Approximate Nearest Neighbor (ANN)
  - IVF (Inverted File Index): í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ê²€ìƒ‰ ê³µê°„ ì¶•ì†Œ
  - HNSW (Hierarchical Navigable Small World): ê·¸ë˜í”„ ê¸°ë°˜ ê²€ìƒ‰
  - PQ (Product Quantization): ë²¡í„° ì••ì¶•

**ê²€ìƒ‰ ì„±ëŠ¥**:
```
Wikipedia 21M ë¬¸ì„œ ê¸°ì¤€:
- Exact Search (Flat Index): ~2ì´ˆ/query
- Approximate Search (HNSW): ~10ms/query (200ë°° ë¹ ë¦„)
- Recall@5: 95%+ (ì •í™•ë„ ì†ì‹¤ ë¯¸ë¯¸)
```

### 2. Generator: BART

**ì—­í• **: ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ì¿¼ë¦¬ë¥¼ ì¡°í•©í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ ìƒì„±

**êµ¬ì¡°**: Seq2Seq Transformer (Encoder-Decoder)

```python
# ì…ë ¥ í˜•ì‹
input = concat([
    query,
    "[SEP]",
    retrieved_document
])

# ì˜ˆì‹œ
input = "Who wrote Python? [SEP] Python was created by Guido van Rossum
         in 1991 at the National Research Institute for Mathematics..."

# BART Encoder
encoder_hidden = BART_encoder(input)  # [seq_len, 1024]

# BART Decoder (Auto-regressive)
output = BART_decoder(
    encoder_hidden=encoder_hidden,
    decoder_input=previous_tokens  # "Guido", "van", "Rossum", ...
)
```

**ì™œ BARTì¸ê°€?**

| ëª¨ë¸ | ì•„í‚¤í…ì²˜ | ì‚¬ì „í•™ìŠµ | ì¥ì  | ë‹¨ì  |
|------|---------|---------|------|------|
| **BART** | Encoder-Decoder | Denoising | ë…¸ì´ì¦ˆì— ê°•ê±´, ìƒì„± í’ˆì§ˆ ìš°ìˆ˜ | ì¶”ë¡  ëŠë¦¼ |
| GPT-2 | Decoder-only | LM | ì¶”ë¡  ë¹ ë¦„ | ì–‘ë°©í–¥ ë¬¸ë§¥ ë¶€ì¡± |
| T5 | Encoder-Decoder | Span Corruption | ë²”ìš©ì„± ë†’ìŒ | ë™ì¼ í¬ê¸°ì—ì„œ BARTë³´ë‹¤ ì„±ëŠ¥ ë‚®ìŒ |

**BARTì˜ Denoising ì‚¬ì „í•™ìŠµ**:
```python
# ì›ë³¸ ë¬¸ì¥
"Python was created by Guido van Rossum in 1991."

# Corruption (Noise ì¶”ê°€)
- Token Masking: "Python was [MASK] by Guido van [MASK] in 1991."
- Token Deletion: "Python created Guido van Rossum 1991."
- Sentence Permutation: "in 1991. Python was created by Guido van Rossum"

# í•™ìŠµ ëª©í‘œ: ë…¸ì´ì¦ˆ ì œê±° í›„ ì›ë³¸ ë³µì›
â†’ ê²€ìƒ‰ëœ ë¬¸ì„œì— ë…¸ì´ì¦ˆ(irrelevant ì •ë³´)ê°€ ìˆì–´ë„ í•µì‹¬ë§Œ ì¶”ì¶œ ê°€ëŠ¥
```

### 3. ë‘ ê°€ì§€ Marginalization ë°©ì‹

RAGì˜ í•µì‹¬ ì•„ì´ë””ì–´: **ì—¬ëŸ¬ ë¬¸ì„œ í›„ë³´ë¥¼ í™•ë¥ ì ìœ¼ë¡œ ì¡°í•©**

#### RAG-Sequence

**íŠ¹ì§•**: ì „ì²´ ë‹µë³€ ìƒì„±ì— ë™ì¼í•œ ë¬¸ì„œ ì‚¬ìš©

**ìˆ˜ì‹**:
```
p(y|x) â‰ˆ Î£_{z âˆˆ top-k} p_Î·(z|x) Ã— p_Î¸(y|x, z)
```

**ë™ì‘ ë°©ì‹**:
```python
# ê° ë¬¸ì„œì— ëŒ€í•´ ë…ë¦½ì ìœ¼ë¡œ ì „ì²´ ë‹µë³€ ìƒì„±
for doc in top_k_docs:
    answer = generate_full_answer(query, doc)
    score = retrieval_prob(doc) Ã— generation_prob(answer|query, doc)
    candidates.append((answer, score))

# í™•ë¥ ë¡œ ê°€ì¤‘ í‰ê·  (ë˜ëŠ” ìµœëŒ“ê°’)
final_answer = weighted_combination(candidates)
```

**ì˜ˆì‹œ**:
```
Query: "Pythonì˜ ì°½ì‹œìëŠ”?"

Document 1 (p=0.6): "Guido van Rossum created Python"
  â†’ Answer: "Guido van Rossum" (prob=0.6 Ã— 0.9 = 0.54)

Document 2 (p=0.3): "Python was developed by Guido"
  â†’ Answer: "Guido van Rossum" (prob=0.3 Ã— 0.85 = 0.255)

Document 3 (p=0.1): "Rossum is a Dutch programmer"
  â†’ Answer: "Rossum" (prob=0.1 Ã— 0.7 = 0.07)

Final: "Guido van Rossum" (highest probability)
```

**ì¥ì **:
- âœ… ì¼ê´€ì„± ìˆëŠ” ë‹µë³€ (í•˜ë‚˜ì˜ ë¬¸ì„œì— ì§‘ì¤‘)
- âœ… í‘œì¤€ beam search ì‚¬ìš© ê°€ëŠ¥
- âœ… ì¶”ë¡  ì†ë„ ë¹ ë¦„

**ë‹¨ì **:
- âŒ ì—¬ëŸ¬ ë¬¸ì„œ ì •ë³´ ì¡°í•© ì–´ë ¤ì›€
- âŒ í•œ ë¬¸ì„œì—ë§Œ ê³¼ë„í•˜ê²Œ ì˜ì¡´

#### RAG-Token

**íŠ¹ì§•**: ê° í† í°ë§ˆë‹¤ ë‹¤ë¥¸ ë¬¸ì„œ ì°¸ì¡° ê°€ëŠ¥

**ìˆ˜ì‹**:
```
p(y|x) â‰ˆ Î _{i=1}^{|y|} Î£_{z âˆˆ top-k} p_Î·(z|x) Ã— p_Î¸(y_i|x, z, y_{1:i-1})
```

**ë™ì‘ ë°©ì‹**:
```python
# ê° í† í° ìƒì„± ì‹œë§ˆë‹¤ ëª¨ë“  ë¬¸ì„œë¥¼ ê³ ë ¤
for i in range(answer_length):
    token_probs = {}

    for doc in top_k_docs:
        # ì´ ë¬¸ì„œë¥¼ ì‚¬ìš©í–ˆì„ ë•Œ ë‹¤ìŒ í† í° í™•ë¥ 
        prob = retrieval_prob(doc) Ã— p(token_i | query, doc, prev_tokens)
        token_probs[token] += prob

    # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í† í° ì„ íƒ
    next_token = argmax(token_probs)
    prev_tokens.append(next_token)
```

**ì˜ˆì‹œ**:
```
Query: "Pythonì˜ ì°½ì‹œìì™€ ì—°ë„ëŠ”?"

Token 1 "Guido":
  - Doc1 (Python creator): p=0.6 Ã— 0.9 = 0.54
  - Doc2 (Guido bio): p=0.3 Ã— 0.8 = 0.24
  â†’ Choose "Guido"

Token 2 "van":
  - Doc1: p=0.6 Ã— 0.85 = 0.51
  - Doc2: p=0.3 Ã— 0.82 = 0.246
  â†’ Choose "van"

Token 3 "Rossum":
  - Doc1: p=0.6 Ã— 0.88 = 0.528
  â†’ Choose "Rossum"

Token 4 "created":
  - Doc1: p=0.6 Ã— 0.7 = 0.42
  â†’ Choose "created"

Token 5 "Python":
  - Doc1: p=0.6 Ã— 0.9 = 0.54
  â†’ Choose "Python"

Token 6 "in":
  - Doc3 (Timeline): p=0.1 Ã— 0.95 = 0.095  â† ë¬¸ì„œ ì „í™˜!
  â†’ Choose "in"

Token 7 "1991":
  - Doc3: p=0.1 Ã— 0.98 = 0.098  â† ë‹¤ë¥¸ ë¬¸ì„œ ì‚¬ìš©
  â†’ Choose "1991"

Final: "Guido van Rossum created Python in 1991"
```

**ì¥ì **:
- âœ… ì—¬ëŸ¬ ë¬¸ì„œì˜ ì •ë³´ë¥¼ í† í° ë‹¨ìœ„ë¡œ ì¡°í•©
- âœ… ë” í’ë¶€í•˜ê³  ë‹¤ì–‘í•œ ë‹µë³€ ìƒì„±

**ë‹¨ì **:
- âŒ ê³„ì‚° ë¹„ìš© ë†’ìŒ (ê° í† í°ë§ˆë‹¤ Kê°œ ë¬¸ì„œ í‰ê°€)
- âŒ ë¬¸ì„œ ê°„ ì „í™˜ ì‹œ ì¼ê´€ì„± ì €í•˜ ê°€ëŠ¥

#### ë¹„êµ í‘œ

| íŠ¹ì„± | RAG-Sequence | RAG-Token |
|------|-------------|-----------|
| Marginalization | ë‹µë³€ ì „ì²´ì— ëŒ€í•´ | ê° í† í°ë§ˆë‹¤ |
| ë¬¸ì„œ ì‚¬ìš© | 1ê°œ ë¬¸ì„œì— ì§‘ì¤‘ | ì—¬ëŸ¬ ë¬¸ì„œ ì¡°í•© |
| ì¼ê´€ì„± | ë†’ìŒ | ì¤‘ê°„ |
| ë‹¤ì–‘ì„± | ë‚®ìŒ | ë†’ìŒ |
| ê³„ì‚° ë¹„ìš© | ë‚®ìŒ | ë†’ìŒ (Kë°°) |
| Beam Search | í‘œì¤€ ë°©ì‹ | ìˆ˜ì • í•„ìš” |
| ì í•© íƒœìŠ¤í¬ | Factoid QA | ìƒì„±, ìš”ì•½ |

## ğŸ”¬ í•™ìŠµ ë° ì¶”ë¡  ìƒì„¸

### í•™ìŠµ ê³¼ì •

**ì†ì‹¤ í•¨ìˆ˜**: Negative Marginal Log-Likelihood

```python
# RAG-Sequence
Loss = -Î£_j log [ Î£_{z âˆˆ top-k} p_Î·(z|x_j) Ã— p_Î¸(y_j|x_j, z) ]

# RAG-Token
Loss = -Î£_j log [ Î _{i=1}^{|y_j|} Î£_{z âˆˆ top-k} p_Î·(z|x_j) Ã— p_Î¸(y_{j,i}|x_j, z, y_{j,1:i-1}) ]
```

**í•µì‹¬ ì„¤ê³„ ê²°ì •**:

**1. Document Encoder ê³ ì • (Frozen)**

```python
# Document EncoderëŠ” DPR ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜ ê·¸ëŒ€ë¡œ ì‚¬ìš©
for param in document_encoder.parameters():
    param.requires_grad = False  # í•™ìŠµí•˜ì§€ ì•ŠìŒ

# Query Encoder + Generatorë§Œ í•™ìŠµ
for param in query_encoder.parameters():
    param.requires_grad = True

for param in bart_generator.parameters():
    param.requires_grad = True
```

**ì™œ ê³ ì •í•˜ëŠ”ê°€?**

| ì¥ì  | ë‹¨ì  |
|------|------|
| âœ… í•™ìŠµ ì¤‘ ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ë¶ˆí•„ìš” | âŒ Document í‘œí˜„ì´ downstream taskì— ìµœì í™” ì•ˆ ë¨ |
| âœ… ê³„ì‚° ë¹„ìš© ëŒ€í­ ì ˆê° (10ë°°â†“) | âŒ Query-Document ë¶ˆê· í˜• ê°€ëŠ¥ì„± |
| âœ… í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ | âŒ ì´ë¡ ì  ìµœì í•´ëŠ” ì•„ë‹˜ |

**ì‹¤í—˜ ê²°ê³¼**:
```
Document Encoder ê³ ì • vs í•™ìŠµ:
- ì„±ëŠ¥ ì°¨ì´: < 1% (ë¯¸ë¯¸í•¨)
- í•™ìŠµ ì‹œê°„: 10ë°° ì°¨ì´
- ë©”ëª¨ë¦¬: 21M ë¬¸ì„œ ì¬ì¸ì½”ë”© ë¶ˆí•„ìš”
â†’ ê³ ì •ì´ ì‹¤ìš©ì ìœ¼ë¡œ ë” ìœ ë¦¬
```

**2. End-to-End í•™ìŠµ**

```python
# Gradient Flow
Loss â†’ Generator (BART) â†’ [gradient flows]
                 â†“
            Query Encoder â†’ [gradient flows]
                 â†“
            Retrieval Score p_Î·(z|x)
```

**íŠ¹ì§•**:
- Retrieverì— ëª…ì‹œì ì¸ supervision ì—†ìŒ (ì–´ë–¤ ë¬¸ì„œê°€ ì •ë‹µì¸ì§€ ë ˆì´ë¸” ë¶ˆí•„ìš”)
- Generatorì˜ í”¼ë“œë°±ì„ í†µí•´ ì•”ë¬µì ìœ¼ë¡œ ê²€ìƒ‰ ëŠ¥ë ¥ í–¥ìƒ
- "ì´ ë¬¸ì„œë¥¼ ì‚¬ìš©í–ˆì„ ë•Œ ì •ë‹µ ìƒì„±ì´ ì˜ ë¨" â†’ Retrieverê°€ ê·¸ ë¬¸ì„œ ì„ í˜¸í•˜ë„ë¡ í•™ìŠµ

### í•™ìŠµ ì•Œê³ ë¦¬ì¦˜

```python
class RAGTrainer:
    def __init__(self, retriever, generator, index):
        self.query_encoder = retriever.query_encoder
        self.doc_encoder = retriever.doc_encoder  # Frozen
        self.generator = generator
        self.index = index  # FAISS index

        # Document EncoderëŠ” ê³ ì •
        self.doc_encoder.eval()
        for param in self.doc_encoder.parameters():
            param.requires_grad = False

        # Query Encoder + Generator í•™ìŠµ
        self.optimizer = AdamW([
            {'params': self.query_encoder.parameters(), 'lr': 1e-5},
            {'params': self.generator.parameters(), 'lr': 3e-5}
        ])

    def train_step(self, batch):
        queries = batch['questions']  # ["What is Python?", ...]
        answers = batch['answers']    # ["A programming language", ...]

        # 1. Retrieve Top-K documents
        query_vectors = self.query_encoder(queries)  # [batch, 768]
        doc_scores, doc_ids = self.index.search(query_vectors, k=10)
        retrieved_docs = self.get_documents(doc_ids)  # [batch, k, doc_len]

        # 2. Compute retrieval probabilities
        retrieval_probs = softmax(doc_scores, dim=-1)  # [batch, k]

        # 3. Generate answers for each document
        generation_probs = []
        for k in range(10):
            inputs = self.concat_inputs(queries, retrieved_docs[:, k, :])
            outputs = self.generator(inputs, labels=answers)

            # p(y|x,z_k)
            gen_prob = exp(-outputs.loss)  # Convert loss to probability
            generation_probs.append(gen_prob)

        generation_probs = torch.stack(generation_probs, dim=1)  # [batch, k]

        # 4. Marginalize (RAG-Sequence)
        marginal_prob = (retrieval_probs * generation_probs).sum(dim=1)

        # 5. Compute loss
        loss = -torch.log(marginal_prob + 1e-10).mean()

        # 6. Backprop (only Query Encoder + Generator)
        loss.backward()
        self.optimizer.step()
        self.optimizer.zero_grad()

        return loss.item()
```

### ì¶”ë¡  (Decoding)

#### RAG-Token Decoding

**ì¥ì **: í‘œì¤€ beam search ì‚¬ìš© ê°€ëŠ¥

```python
def rag_token_generate(query, top_k_docs, k=10, beam_size=4):
    # ê° ë””ì½”ë”© ìŠ¤í…ì—ì„œ ëª¨ë“  ë¬¸ì„œë¥¼ marginalize
    for step in range(max_length):
        # í˜„ì¬ê¹Œì§€ ìƒì„±ëœ í† í°: y_{1:i-1}

        # ë‹¤ìŒ í† í° í™•ë¥  ê³„ì‚°
        next_token_probs = torch.zeros(vocab_size)

        for doc, doc_prob in zip(top_k_docs, retrieval_probs):
            # p(y_i | x, z, y_{1:i-1})
            logits = generator(query, doc, prev_tokens)
            token_probs = softmax(logits)

            # Marginalize: Î£ p(z|x) Ã— p(y_i|x,z,y_{1:i-1})
            next_token_probs += doc_prob * token_probs

        # Beam search ì—…ë°ì´íŠ¸
        top_tokens = next_token_probs.topk(beam_size)
        beams = update_beams(beams, top_tokens)

    return best_beam
```

#### RAG-Sequence Decoding

**ë¬¸ì œ**: ê° ë¬¸ì„œë§ˆë‹¤ ë‹¤ë¥¸ ë‹µë³€ ìƒì„± â†’ ì–´ë–»ê²Œ ì¡°í•©?

**ë°©ë²• 1: Thorough Decoding (ì •í™•í•˜ì§€ë§Œ ëŠë¦¼)**

```python
def rag_sequence_thorough_decode(query, top_k_docs, beam_size=4):
    all_hypotheses = []

    # 1. ê° ë¬¸ì„œì— ëŒ€í•´ ë…ë¦½ì ìœ¼ë¡œ beam search
    for doc, doc_prob in zip(top_k_docs, retrieval_probs):
        # ì´ ë¬¸ì„œë§Œ ì‚¬ìš©í•˜ì—¬ ìƒì„±
        beams = beam_search(query, doc, beam_size)

        for hypothesis in beams:
            all_hypotheses.append({
                'text': hypothesis.text,
                'doc': doc,
                'doc_prob': doc_prob,
                'gen_prob': hypothesis.prob
            })

    # 2. ëª¨ë“  ê°€ì„¤ì˜ í•©ì§‘í•© Y ìƒì„±
    unique_hypotheses = set([h['text'] for h in all_hypotheses])

    # 3. ëˆ„ë½ëœ ê°€ì„¤ì— ëŒ€í•´ ì¶”ê°€ forward pass
    for hyp_text in unique_hypotheses:
        for doc in top_k_docs:
            if not exists(hyp_text, doc):
                # p(y|x, doc) ê³„ì‚°
                prob = compute_generation_prob(hyp_text, query, doc)
                all_hypotheses.append({...})

    # 4. Marginalizeí•˜ì—¬ ìµœì¢… í™•ë¥  ê³„ì‚°
    final_probs = {}
    for hyp in unique_hypotheses:
        prob = sum([
            h['doc_prob'] * h['gen_prob']
            for h in all_hypotheses
            if h['text'] == hyp
        ])
        final_probs[hyp] = prob

    # 5. ìµœê³  í™•ë¥  ê°€ì„¤ ë°˜í™˜
    return max(final_probs, key=final_probs.get)
```

**ì‹œê°„ ë³µì¡ë„**: O(K Ã— beam_size + |Y| Ã— K)
- K=10, beam_size=4 â†’ 40ê°œ ê°€ì„¤ ìƒì„±
- |Y| ìµœì•…ì˜ ê²½ìš° 40ê°œ (ëª¨ë‘ ë‹¤ë¦„) â†’ 400ë²ˆ forward pass
- **ë§¤ìš° ëŠë¦¼!**

**ë°©ë²• 2: Fast Decoding (ë¹ ë¥´ì§€ë§Œ ê·¼ì‚¬)**

```python
def rag_sequence_fast_decode(query, top_k_docs, beam_size=4):
    hypotheses_by_doc = {}

    # 1. ê° ë¬¸ì„œë§ˆë‹¤ beam search
    for i, (doc, doc_prob) in enumerate(zip(top_k_docs, retrieval_probs)):
        beams = beam_search(query, doc, beam_size)
        hypotheses_by_doc[i] = beams

    # 2. ê·¼ì‚¬: beamì— ë‚˜íƒ€ë‚˜ì§€ ì•Šì€ ê°€ì„¤ì˜ í™•ë¥ ì„ 0ìœ¼ë¡œ ê°€ì •
    #    p(y|x,z_i) â‰ˆ 0  if y not in beam(x, z_i)

    final_probs = {}
    for doc_idx, beams in hypotheses_by_doc.items():
        doc_prob = retrieval_probs[doc_idx]

        for hypothesis in beams:
            if hypothesis.text not in final_probs:
                final_probs[hypothesis.text] = 0

            # ì´ ë¬¸ì„œì—ì„œ ì´ ê°€ì„¤ì˜ í™•ë¥ ë§Œ ë”í•¨
            final_probs[hypothesis.text] += doc_prob * hypothesis.prob

    return max(final_probs, key=final_probs.get)
```

**ì‹œê°„ ë³µì¡ë„**: O(K Ã— beam_size)
- Thoroughì˜ 10ë°° ë¹ ë¦„
- ì‹¤í—˜ ê²°ê³¼ ì„±ëŠ¥ ì°¨ì´ < 2%

### í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°

```python
# Optimizer
optimizer = AdamW
learning_rate_query_encoder = 1e-5
learning_rate_generator = 3e-5
weight_decay = 0.01
warmup_steps = 500

# Training
batch_size = 128
max_epochs = 10  # ëŒ€ë¶€ë¶„ íƒœìŠ¤í¬ì—ì„œ ì¡°ê¸° ìˆ˜ë ´
gradient_accumulation_steps = 2
max_grad_norm = 1.0  # Gradient clipping

# Retrieval
num_retrieved_docs = 10  # Training ì‹œ
num_retrieved_docs_inference = 5~50  # Task-dependent

# Generation
max_input_length = 512  # Query + Document
max_output_length = 50  # QA tasks
max_output_length = 256  # Generation tasks

# Regularization
dropout = 0.1
label_smoothing = 0.1  # Generatorì—ë§Œ ì ìš©
```

## ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ë¶„ì„

### 1. Open-Domain QA ì„±ëŠ¥

**ë°ì´í„°ì…‹**:
- **Natural Questions (NQ)**: Google ê²€ìƒ‰ ì¿¼ë¦¬ ê¸°ë°˜, 79k train / 8.7k dev
- **TriviaQA**: í€´ì¦ˆ ì§ˆë¬¸, 78k train / 8.8k dev
- **WebQuestions (WQ)**: Freebase ê¸°ë°˜, 3.4k train / 2k dev
- **CuratedTREC**: TREC QA ë°ì´í„°, 1.4k train / 694 dev

**ê²°ê³¼**:

| Model | Type | Params | NQ | TriviaQA | WQ | TREC |
|-------|------|--------|-------|----------|-----|------|
| T5-11B | Closed Book | 11B | 34.5 | 50.1 | 37.4 | - |
| T5-11B + SSM | Closed Book | 11B | 36.6 | - | - | - |
| DPR | Open Book (Extract) | - | 41.5 | 57.9 | 41.1 | - |
| **RAG-Sequence** | **Hybrid** | **626M** | **44.5** | **56.8** | **45.2** | **68.0** |
| **RAG-Token** | **Hybrid** | **626M** | **44.1** | **68.0** | **45.5** | **63.2** |

**í•µì‹¬ ë°œê²¬**:

1. **íŒŒë¼ë¯¸í„° íš¨ìœ¨ì„±**:
   ```
   RAG-626M > T5-11B (17.6ë°° ì‘ì€ ëª¨ë¸ë¡œ ë” ì¢‹ì€ ì„±ëŠ¥)
   - NQ: 44.5 vs 34.5 (+10.0)
   - WQ: 45.2 vs 37.4 (+7.8)
   ```

2. **Parametric + Non-Parametric ì‹œë„ˆì§€**:
   ```
   RAG (44.5) > DPR (41.5) > T5 (34.5)
   - RAG = DPR retrieval + BART generation
   - ê°ê°ë³´ë‹¤ ì¡°í•©ì´ ìš°ìˆ˜
   ```

3. **RAG-Sequence vs RAG-Token**:
   ```
   - Factoid QA (NQ, WQ): ë¹„ìŠ·í•¨
   - ì§€ì‹ ì§‘ì•½ì  (TriviaQA): RAG-Tokenì´ ìš°ìˆ˜ (68.0 vs 56.8)
     â†’ ì—¬ëŸ¬ ë¬¸ì„œ ì¡°í•© ëŠ¥ë ¥ì´ ì¤‘ìš”
   ```

### 2. ìƒì„± íƒœìŠ¤í¬ ì„±ëŠ¥

**MSMARCO NLG (ìì—°ì–´ ìƒì„±)**:

| Model | BLEU-1 | ROUGE-L | Human Rating |
|-------|--------|---------|--------------|
| BART | 34.2 | 22.1 | 3.2/5 |
| RAG-Sequence | **37.6** | **24.8** | **4.1/5** |

**Jeopardy Question Generation**:

| Model | Factuality | Specificity | Human Pref |
|-------|-----------|-------------|------------|
| BART | 7.1% | 16.8% | 25.3% |
| RAG-Token | **42.7%** | **37.4%** | **52.6%** |

**ì‹¤ì œ ì˜ˆì‹œ**:

```
Category: "SCIENCE"
Answer: "DNA"

BART (Closed-Book):
"This molecule carries genetic information"
â†’ ì¼ë°˜ì ì´ì§€ë§Œ êµ¬ì²´ì„± ë¶€ì¡±

RAG (with retrieval):
"Discovered by Watson and Crick in 1953, this double helix
 molecule carries genetic instructions for development"
â†’ êµ¬ì²´ì ì´ê³  ì‚¬ì‹¤ ê¸°ë°˜
```

### 3. Fact Verification (FEVER)

**íƒœìŠ¤í¬**: ì£¼ì¥(claim)ì´ ì°¸ì¸ì§€ ê±°ì§“ì¸ì§€ ê²€ì¦

| Model | Accuracy | Label Acc |
|-------|----------|-----------|
| BERT-baseline | 71.6% | 89.2% |
| KGAT | 74.1% | 91.2% |
| RAG-Sequence | **74.8%** | **92.3%** |

**ì˜ˆì‹œ**:

```
Claim: "The sun is the largest star in the universe"

RAG Process:
1. Retrieve: "The sun is a medium-sized star..." (Wikipedia: Sun)
2. Retrieve: "UY Scuti is one of the largest known stars..." (Wikipedia: List of largest stars)
3. Generate: REFUTED (confidence: 0.92)
4. Evidence: [doc1, doc2]
```

### 4. Ablation Studies

#### Top-K ë¬¸ì„œ ìˆ˜ ì˜í–¥

**NQ ë°ì´í„°ì…‹**:

| K | RAG-Sequence | RAG-Token | Inference Time |
|---|--------------|-----------|----------------|
| 1 | 38.2 | 37.9 | 1.0x |
| 5 | 43.1 | 42.8 | 2.1x |
| 10 | **44.5** | 44.1 | 3.5x |
| 15 | 44.6 | **44.3** | 5.2x |
| 20 | 44.5 | 44.2 | 7.1x |
| 50 | 44.3 | 43.9 | 18.3x |

**ê²°ë¡ **:
- K=10ì´ ì„±ëŠ¥/ì†ë„ ìµœì  ê· í˜•ì 
- RAG-Sequence: K ì¦ê°€ ì‹œ ê³„ì† í–¥ìƒ (K=50ê¹Œì§€)
- RAG-Token: K=10~15ì—ì„œ í”¼í¬ (ì´í›„ ì •ì²´)

#### Document Encoder ê³ ì • vs í•™ìŠµ

**NQ ë°ì´í„°ì…‹**:

| Document Encoder | NQ Score | Training Time | GPU Memory |
|------------------|----------|---------------|------------|
| Frozen (ë…¼ë¬¸) | 44.5 | 1.0x | 16GB |
| Fine-tuned | 45.1 | 10.2x | 48GB |

**ê²°ë¡ **:
- ì„±ëŠ¥ í–¥ìƒ < 1%
- ë¹„ìš© ì¦ê°€ 10ë°° ì´ìƒ
- **ê³ ì •ì´ ì‹¤ìš©ì **

#### Generator í¬ê¸° ì˜í–¥

| Generator Model | Params | NQ | TriviaQA |
|-----------------|--------|-----|----------|
| BART-base | 140M | 40.2 | 51.3 |
| BART-large | 400M | **44.5** | **56.8** |
| T5-base (ë¹„êµ) | 220M | 42.1 | 53.7 |

#### Retrieval vs Parametric ê¸°ì—¬ë„

**êµ¬ì„± ìš”ì†Œë³„ ë¶„ì„**:

```python
# 1. Parametric-only (retrieval ì—†ìŒ)
BART-400M: NQ 32.1

# 2. Retrieval-only (generation ì—†ìŒ, extractive QA)
DPR: NQ 41.5

# 3. RAG (ê²°í•©)
RAG: NQ 44.5

# ë¶„ì„
Parametric ê¸°ì—¬: 32.1 / 44.5 = 72%
Retrieval ê¸°ì—¬: 12.4 / 44.5 = 28%
ì‹œë„ˆì§€ íš¨ê³¼: 44.5 - max(32.1, 41.5) = +3.0
```

### 5. ìƒì„± ë‹¤ì–‘ì„± ë¶„ì„

**Tri-gram Diversity** (ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘í•¨):

| Model | MS-MARCO | Jeopardy |
|-------|----------|----------|
| Gold (Human) | 90.0% | 95.2% |
| RAG-Sequence | 53.8% | 61.3% |
| RAG-Token | **46.8%** | **58.7%** |
| BART | 32.4% | 41.2% |

**í•´ì„**:
- RAGê°€ BARTë³´ë‹¤ ë‹¤ì–‘í•œ í‘œí˜„ ìƒì„± (ê²€ìƒ‰ ë¬¸ì„œì—ì„œ ë‹¤ì–‘í•œ í‘œí˜„ í•™ìŠµ)
- í•˜ì§€ë§Œ ì—¬ì „íˆ ì¸ê°„ë³´ë‹¤ëŠ” ë°˜ë³µì 
- RAG-Tokenì´ RAG-Sequenceë³´ë‹¤ ëœ ë‹¤ì–‘ (ì—¬ëŸ¬ ë¬¸ì„œ ì¡°í•© ì‹œ ì¼ê´€ì„± ìœ ì§€)

### 6. ì—ëŸ¬ ë¶„ì„

**NQì—ì„œ ì˜¤ë‹µ ìœ í˜•** (100ê°œ ìƒ˜í”Œ ë¶„ì„):

| ì—ëŸ¬ ìœ í˜• | ë¹„ìœ¨ | ì˜ˆì‹œ |
|----------|------|------|
| ê²€ìƒ‰ ì‹¤íŒ¨ | 38% | ê´€ë ¨ ë¬¸ì„œê°€ Top-Kì— ì—†ìŒ |
| ì¶”ì¶œ ì‹¤íŒ¨ | 27% | ë¬¸ì„œì—ëŠ” ë‹µì´ ìˆì§€ë§Œ ìƒì„± ëª»í•¨ |
| ëª¨í˜¸í•œ ì§ˆë¬¸ | 18% | "ê·¸ëŠ” ëˆ„êµ¬ì¸ê°€?" (ì§€ì¹­ ë¶ˆëª…í™•) |
| ìµœì‹  ì •ë³´ | 12% | Wikipediaê°€ outdated |
| ì¶”ë¡  í•„ìš” | 5% | ë‹¤ë‹¨ê³„ ì¶”ë¡  ì‹¤íŒ¨ |

**ì‹¤ì œ ì‹¤íŒ¨ ì‚¬ë¡€**:

```
Query: "Who is the current president of France?"
(í‰ê°€ ì‹œì : 2020)

Top Retrieved Docs:
1. "FranÃ§ois Hollande was president from 2012-2017" (outdated)
2. "Emmanuel Macron won the 2017 election" (ì •ë‹µ ì•”ì‹œ)
3. "France is a republic..." (irrelevant)

Generated: "FranÃ§ois Hollande"  âŒ
Correct: "Emmanuel Macron"

ì›ì¸: ê²€ìƒ‰ëœ ë¬¸ì„œ 1ì˜ í™•ë¥ ì´ ê°€ì¥ ë†’ìŒ (outdated ì •ë³´)
```

## ğŸ’» ì‹¤ì „ êµ¬í˜„ ê°€ì´ë“œ

### 1. ê¸°ë³¸ êµ¬í˜„ (HuggingFace)

```python
from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration
import torch

class BasicRAG:
    def __init__(self, model_name="facebook/rag-token-nq"):
        # Tokenizer
        self.tokenizer = RagTokenizer.from_pretrained(model_name)

        # Retriever (DPR + FAISS)
        self.retriever = RagRetriever.from_pretrained(
            model_name,
            index_name="exact",  # or "compressed" for smaller index
            use_dummy_dataset=False
        )

        # Generator (BART)
        self.model = RagTokenForGeneration.from_pretrained(
            model_name,
            retriever=self.retriever
        )

        # GPU ì‚¬ìš©
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model.to(self.device)

    def answer(self, question, num_return_sequences=1, num_beams=4):
        # Tokenize
        inputs = self.tokenizer(question, return_tensors="pt")
        inputs = {k: v.to(self.device) for k, v in inputs.items()}

        # Generate
        with torch.no_grad():
            outputs = self.model.generate(
                input_ids=inputs["input_ids"],
                attention_mask=inputs["attention_mask"],
                num_return_sequences=num_return_sequences,
                num_beams=num_beams,
                max_length=50,
                early_stopping=True
            )

        # Decode
        answers = [
            self.tokenizer.decode(output, skip_special_tokens=True)
            for output in outputs
        ]

        return answers[0] if num_return_sequences == 1 else answers

    def answer_with_sources(self, question, num_docs=5):
        # Tokenize
        inputs = self.tokenizer(question, return_tensors="pt")
        inputs = {k: v.to(self.device) for k, v in inputs.items()}

        # Retrieve documents
        question_hidden_states = self.model.question_encoder(
            inputs["input_ids"]
        )[0]

        docs_dict = self.retriever(
            inputs["input_ids"].cpu().numpy(),
            question_hidden_states.cpu().detach().numpy(),
            return_tensors="pt",
            n_docs=num_docs
        )

        # Generate
        with torch.no_grad():
            outputs = self.model.generate(
                input_ids=inputs["input_ids"],
                context_input_ids=docs_dict["context_input_ids"].to(self.device),
                context_attention_mask=docs_dict["context_attention_mask"].to(self.device),
                doc_scores=docs_dict["doc_scores"].to(self.device),
                num_beams=4,
                max_length=50
            )

        answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # ê²€ìƒ‰ëœ ë¬¸ì„œ ì •ë³´
        sources = []
        for i in range(num_docs):
            sources.append({
                "title": docs_dict["retrieved_doc_title"][0][i],
                "text": self.tokenizer.decode(
                    docs_dict["context_input_ids"][0][i],
                    skip_special_tokens=True
                ),
                "score": docs_dict["doc_scores"][0][i].item()
            })

        return {
            "answer": answer,
            "sources": sources
        }

# ì‚¬ìš© ì˜ˆì‹œ
rag = BasicRAG()

# ê°„ë‹¨í•œ ì§ˆë¬¸
answer = rag.answer("Who created Python?")
print(answer)  # "Guido van Rossum"

# ì¶œì²˜ í¬í•¨
result = rag.answer_with_sources("When was Python created?")
print(f"Answer: {result['answer']}")
print(f"Sources:")
for i, src in enumerate(result['sources'][:3]):
    print(f"  {i+1}. {src['title']} (score: {src['score']:.3f})")
```

### 2. ì»¤ìŠ¤í…€ ë¬¸ì„œ ì¸ë±ìŠ¤ êµ¬ì¶•

```python
from datasets import Dataset
import numpy as np
from transformers import DPRContextEncoder, DPRContextEncoderTokenizer
import faiss

class CustomRAGIndex:
    def __init__(self):
        # DPR Context Encoder for documents
        self.ctx_encoder = DPRContextEncoder.from_pretrained(
            "facebook/dpr-ctx_encoder-single-nq-base"
        )
        self.ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(
            "facebook/dpr-ctx_encoder-single-nq-base"
        )
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.ctx_encoder.to(self.device)

    def build_index(self, documents, output_dir="./my_rag_index"):
        """
        documents: List[Dict]
            [
                {"title": "Python", "text": "Python is a programming language..."},
                {"title": "Java", "text": "Java is a..."},
                ...
            ]
        """
        # 1. ë¬¸ì„œë¥¼ Dataset í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        dataset = Dataset.from_dict({
            "title": [doc["title"] for doc in documents],
            "text": [doc["text"] for doc in documents]
        })

        # 2. ë¬¸ì„œ ë²¡í„°í™”
        embeddings = []
        batch_size = 16

        for i in range(0, len(documents), batch_size):
            batch = documents[i:i+batch_size]
            texts = [f"{doc['title']} {doc['text']}" for doc in batch]

            # Tokenize
            inputs = self.ctx_tokenizer(
                texts,
                padding=True,
                truncation=True,
                max_length=256,
                return_tensors="pt"
            ).to(self.device)

            # Encode
            with torch.no_grad():
                outputs = self.ctx_encoder(**inputs)
                embeddings.append(outputs.pooler_output.cpu().numpy())

            if (i // batch_size) % 100 == 0:
                print(f"Encoded {i}/{len(documents)} documents")

        embeddings = np.vstack(embeddings)  # [num_docs, 768]

        # 3. FAISS ì¸ë±ìŠ¤ êµ¬ì¶•
        dimension = embeddings.shape[1]  # 768

        # Flat index (ì •í™•í•˜ì§€ë§Œ ëŠë¦¼)
        # index = faiss.IndexFlatIP(dimension)

        # HNSW index (ë¹ ë¥´ê³  ì •í™•í•¨)
        index = faiss.IndexHNSWFlat(dimension, 128)
        index.hnsw.efConstruction = 200
        index.hnsw.efSearch = 128

        # Normalize embeddings (inner product â†’ cosine similarity)
        faiss.normalize_L2(embeddings)

        # Add to index
        index.add(embeddings)

        # 4. ì €ì¥
        import os
        os.makedirs(output_dir, exist_ok=True)

        faiss.write_index(index, f"{output_dir}/index.faiss")
        dataset.save_to_disk(f"{output_dir}/passages")

        print(f"Index built: {len(documents)} documents")
        print(f"Saved to {output_dir}")

        return index, dataset

# ì‚¬ìš© ì˜ˆì‹œ
documents = [
    {
        "title": "Python Programming",
        "text": "Python is a high-level programming language created by Guido van Rossum in 1991."
    },
    {
        "title": "Java Programming",
        "text": "Java is a programming language developed by James Gosling at Sun Microsystems in 1995."
    },
    # ... ìˆ˜ì²œ~ìˆ˜ë°±ë§Œ ê°œ ë¬¸ì„œ
]

indexer = CustomRAGIndex()
index, dataset = indexer.build_index(documents, "./my_custom_index")
```

### 3. ì»¤ìŠ¤í…€ ì¸ë±ìŠ¤ë¡œ RAG ì‚¬ìš©

```python
class CustomRAG:
    def __init__(self, index_path="./my_custom_index"):
        from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration

        # Tokenizer & Model
        self.tokenizer = RagTokenizer.from_pretrained("facebook/rag-token-nq")
        self.model = RagTokenForGeneration.from_pretrained("facebook/rag-token-nq")

        # Custom Retriever
        self.retriever = RagRetriever.from_pretrained(
            "facebook/rag-token-nq",
            index_name="custom",
            passages_path=f"{index_path}/passages",
            index_path=f"{index_path}/index.faiss"
        )

        # ëª¨ë¸ì— retriever ì—°ê²°
        self.model.set_retriever(self.retriever)

        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model.to(self.device)

    def answer(self, question):
        inputs = self.tokenizer(question, return_tensors="pt")
        inputs = {k: v.to(self.device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = self.model.generate(**inputs, num_beams=4, max_length=50)

        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)

# ì‚¬ìš©
custom_rag = CustomRAG("./my_custom_index")
answer = custom_rag.answer("Who created Python?")
print(answer)
```

### 4. Production ìµœì í™”

```python
class ProductionRAG:
    def __init__(self, index_path, use_gpu=True):
        from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration
        import faiss

        self.tokenizer = RagTokenizer.from_pretrained("facebook/rag-token-nq")
        self.model = RagTokenForGeneration.from_pretrained("facebook/rag-token-nq")

        # GPU ìµœì í™”
        if use_gpu and torch.cuda.is_available():
            self.device = "cuda"
            self.model.to(self.device)
            self.model.half()  # FP16 for faster inference
        else:
            self.device = "cpu"

        # FAISS GPU ì¸ë±ìŠ¤
        cpu_index = faiss.read_index(f"{index_path}/index.faiss")

        if use_gpu:
            res = faiss.StandardGpuResources()
            self.index = faiss.index_cpu_to_gpu(res, 0, cpu_index)
        else:
            self.index = cpu_index

        # Retriever
        self.retriever = RagRetriever.from_pretrained(
            "facebook/rag-token-nq",
            index_name="custom",
            passages_path=f"{index_path}/passages",
            index_path=f"{index_path}/index.faiss"
        )
        self.retriever.index = self.index

        self.model.set_retriever(self.retriever)

        # Caching
        from functools import lru_cache
        self._cached_retrieve = lru_cache(maxsize=10000)(self._retrieve)

    def _retrieve(self, question_hash, n_docs=5):
        # ì‹¤ì œ ê²€ìƒ‰ ìˆ˜í–‰
        inputs = self.tokenizer(question_hash, return_tensors="pt")
        return self.retriever.retrieve(inputs["input_ids"], n_docs=n_docs)

    def answer_batch(self, questions, batch_size=8):
        """ë°°ì¹˜ ì¶”ë¡ ìœ¼ë¡œ throughput í–¥ìƒ"""
        all_answers = []

        for i in range(0, len(questions), batch_size):
            batch = questions[i:i+batch_size]

            # Tokenize
            inputs = self.tokenizer(
                batch,
                padding=True,
                truncation=True,
                return_tensors="pt"
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            # Generate
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    num_beams=4,
                    max_length=50,
                    early_stopping=True
                )

            # Decode
            answers = [
                self.tokenizer.decode(out, skip_special_tokens=True)
                for out in outputs
            ]
            all_answers.extend(answers)

        return all_answers

    def answer_streaming(self, question, num_docs=5):
        """ìŠ¤íŠ¸ë¦¬ë° ìƒì„± (ì‹¤ì‹œê°„ ì‘ë‹µ)"""
        from transformers import TextIteratorStreamer
        from threading import Thread

        inputs = self.tokenizer(question, return_tensors="pt")
        inputs = {k: v.to(self.device) for k, v in inputs.items()}

        streamer = TextIteratorStreamer(
            self.tokenizer,
            skip_prompt=True,
            skip_special_tokens=True
        )

        generation_kwargs = {
            **inputs,
            "streamer": streamer,
            "num_beams": 1,  # Streamingì€ greedyë§Œ ì§€ì›
            "max_length": 50
        }

        # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ìƒì„±
        thread = Thread(target=self.model.generate, kwargs=generation_kwargs)
        thread.start()

        # í† í°ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ yield
        for text in streamer:
            yield text

        thread.join()

# ì‚¬ìš© ì˜ˆì‹œ
prod_rag = ProductionRAG("./my_custom_index", use_gpu=True)

# ë‹¨ì¼ ì¿¼ë¦¬
answer = prod_rag.answer("Who created Python?")

# ë°°ì¹˜ ì²˜ë¦¬
questions = ["Who created Python?", "When was Java created?", ...]
answers = prod_rag.answer_batch(questions, batch_size=16)

# ìŠ¤íŠ¸ë¦¬ë°
for token in prod_rag.answer_streaming("Explain Python"):
    print(token, end="", flush=True)
```

### 5. ì¸ë±ìŠ¤ ì••ì¶• (ë©”ëª¨ë¦¬ ìµœì í™”)

```python
class CompressedRAGIndex:
    @staticmethod
    def compress_index(input_index_path, output_index_path, compression="PQ"):
        """
        FAISS ì¸ë±ìŠ¤ ì••ì¶•
        - Flat: 100GB (21M docs Ã— 768 dim Ã— 4 bytes)
        - IVF+PQ: ~10GB (10ë°° ì••ì¶•)
        - ScalarQuantizer: ~36GB (3ë°° ì••ì¶•)
        """
        import faiss

        # ì›ë³¸ ì¸ë±ìŠ¤ ë¡œë“œ
        index = faiss.read_index(input_index_path)
        d = index.d  # dimension (768)

        if compression == "PQ":
            # Product Quantization
            # 768 dim â†’ 96 subvectors Ã— 8 bits = 96 bytes/vector
            m = 96  # number of subquantizers
            nbits = 8  # bits per subquantizer

            # Train PQ
            compressed = faiss.IndexPQ(d, m, nbits)
            vectors = index.reconstruct_n(0, index.ntotal)
            compressed.train(vectors)
            compressed.add(vectors)

        elif compression == "SQ":
            # Scalar Quantization
            # 768 dim Ã— 1 byte = 768 bytes/vector (vs 3072 bytes in FP32)
            compressed = faiss.IndexScalarQuantizer(
                d,
                faiss.ScalarQuantizer.QT_8bit
            )
            vectors = index.reconstruct_n(0, index.ntotal)
            compressed.train(vectors)
            compressed.add(vectors)

        elif compression == "IVF_PQ":
            # IVF + PQ (ìµœê³  ì••ì¶•ë¥ )
            nlist = 4096  # number of clusters
            m = 96
            nbits = 8

            quantizer = faiss.IndexFlatIP(d)
            compressed = faiss.IndexIVFPQ(quantizer, d, nlist, m, nbits)

            vectors = index.reconstruct_n(0, index.ntotal)
            compressed.train(vectors)
            compressed.add(vectors)
            compressed.nprobe = 32  # search clusters

        # ì €ì¥
        faiss.write_index(compressed, output_index_path)

        # ì••ì¶•ë¥  ë¹„êµ
        import os
        original_size = os.path.getsize(input_index_path) / 1e9  # GB
        compressed_size = os.path.getsize(output_index_path) / 1e9

        print(f"Original: {original_size:.2f} GB")
        print(f"Compressed: {compressed_size:.2f} GB")
        print(f"Compression ratio: {original_size/compressed_size:.1f}x")

        return compressed

# ì‚¬ìš©
CompressedRAGIndex.compress_index(
    "./my_index/index.faiss",
    "./my_index/index_compressed.faiss",
    compression="IVF_PQ"
)
```

## âš ï¸ ë…¼ë¬¸ì˜ í•œê³„ì 

### 1. Retrieval ì˜ì¡´ì„±

**ë¬¸ì œ**: ê´€ë ¨ ë¬¸ì„œê°€ ì¸ë±ìŠ¤ì— ì—†ìœ¼ë©´ ë‹µë³€ ë¶ˆê°€

```python
# ì˜ˆì‹œ: ìµœì‹  ì •ë³´
Query: "Who won the 2024 Olympics 100m?" (2020ë…„ ëª¨ë¸)

Retrieved Docs (Wikipedia 2020):
- "Usain Bolt won gold in 2008, 2012, 2016"
- "100m is a track and field event..."

Generated: "Usain Bolt" âŒ
Correct: "Noah Lyles" (but not in index)
```

**í•´ê²° ë°©ì•ˆ**:
- **Parametric Fallback**: ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ LLM ì§€ì‹ìœ¼ë¡œ ëŒ€ì²´
- **ì£¼ê¸°ì  ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸**: ì‹¤ì‹œê°„ ë‰´ìŠ¤ í¬ë¡¤ë§
- **Confidence Threshold**: ë‚®ì€ ì‹ ë¢°ë„ ì‹œ "I don't know" ë°˜í™˜

### 2. ê¸´ ë¬¸ì„œ ì²˜ë¦¬ì˜ í•œê³„

**ë¬¸ì œ**: 100ë‹¨ì–´ ì²­í¬ë¡œ ë¶„í•  â†’ ë¬¸ë§¥ ì†ì‹¤

```python
# ì›ë³¸ ë¬¸ì„œ (500 words)
"Python was created by Guido van Rossum. ... [ì¤‘ëµ] ...
 He started development in December 1989."

# ì²­í¬ ë¶„í• 
Chunk 1: "Python was created by Guido van Rossum..."
Chunk 2: "...He started development in December 1989."

# ê²€ìƒ‰ ì‹œ "He"ì˜ ì§€ì¹­ ëŒ€ìƒ ë¶ˆëª…í™•
```

**í•´ê²° ë°©ì•ˆ**:
- **Hierarchical Retrieval**: ë¬¸ì„œ â†’ ì„¹ì…˜ â†’ ì²­í¬ (ê³„ì¸µì  ê²€ìƒ‰)
- **Overlapping Chunks**: ì²­í¬ ê°„ 50% ì˜¤ë²„ë©
- **Long-context Models**: Longformer, LED (16k í† í°)

### 3. Retrieval Latency

**ë¬¸ì œ**: ì‹¤ì‹œê°„ ì„œë¹„ìŠ¤ì—ì„œ ê²€ìƒ‰ ì‹œê°„ì´ ë³‘ëª©

```
Latency Breakdown (K=10 ë¬¸ì„œ):
- Retrieval (FAISS): 10-50ms
- Document Encoding: 20-100ms
- Generation: 200-500ms
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Total: 230-650ms (ëª©í‘œ: <100ms)
```

**í•´ê²° ë°©ì•ˆ**:
```python
# 1. GPU FAISS
res = faiss.StandardGpuResources()
gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index)
# 10ms â†’ 2ms

# 2. ë¬¸ì„œ ìºì‹±
from functools import lru_cache

@lru_cache(maxsize=10000)
def retrieve_docs(query_hash):
    return index.search(query_hash, k=10)

# 3. Approximate Search
index.nprobe = 16  # Exact: 128, Fast: 16
# 50ms â†’ 10ms, Recall: 99% â†’ 95%

# 4. Smaller K
k = 5  # instead of 10
# ì„±ëŠ¥ ì €í•˜: ~1%, ì†ë„ í–¥ìƒ: 2ë°°
```

### 4. í¸í–¥ëœ ì§€ì‹ ì†ŒìŠ¤

**ë¬¸ì œ**: Wikipedia ì¤‘ì‹¬ â†’ íŠ¹ì • ë„ë©”ì¸/ì–¸ì–´ì—ì„œ ì„±ëŠ¥ ì €í•˜

```
Wikipedia í¸í–¥:
- ì„œêµ¬ ì¤‘ì‹¬ (ì˜ì–´ ë¬¸ì„œ 6M vs í•œêµ­ì–´ 500K)
- ìœ ëª… ì¸ë¬¼/ì‚¬ê±´ ì¤‘ì‹¬ (í‹ˆìƒˆ ì£¼ì œ ë¶€ì¡±)
- í•™ìˆ ì  ë‚´ìš© ë¶€ì¡± (ë…¼ë¬¸ ë°ì´í„° ì—†ìŒ)
```

**í•´ê²° ë°©ì•ˆ**:
- **Domain-specific Corpus**: ì˜ë£Œ(PubMed), ë²•ë¥ (Case Law), ê¸°ì—…(ë‚´ë¶€ ë¬¸ì„œ)
- **ë‹¤êµ­ì–´ ì¸ë±ìŠ¤**: mDPR + mBART
- **ì „ë¬¸ê°€ íë ˆì´ì…˜**: ê³ í’ˆì§ˆ ë¬¸ì„œ ì„ ë³„

### 5. ë©€í‹°í™‰ ì¶”ë¡  í•œê³„

**ë¬¸ì œ**: ì—¬ëŸ¬ ë‹¨ê³„ ì¶”ë¡  í•„ìš”í•œ ì§ˆë¬¸ì— ì•½í•¨

```
Query: "Who is the spouse of the Python creator?"

Needed reasoning:
1. Python creator = Guido van Rossum
2. Guido van Rossum's spouse = ?

RAG-Sequence:
- Doc1: "Python was created by Guido van Rossum"
- Generate: "Guido van Rossum" âŒ (1ë‹¨ê³„ë§Œ ìˆ˜í–‰)

Correct answer: "Kim Knapp" (requires 2-hop)
```

**í›„ì† ì—°êµ¬**:
- **Self-Ask (2023)**: ì§ˆë¬¸ì„ sub-questionìœ¼ë¡œ ë¶„í•´
- **ReAct (2023)**: Reasoning + Acting loop
- **Chain-of-Thought RAG**: ë‹¨ê³„ë³„ ê²€ìƒ‰ + ì¶”ë¡ 

### 6. ìƒì„± ì œì–´ì˜ ì–´ë ¤ì›€

**ë¬¸ì œ**: ê²€ìƒ‰ ë¬¸ì„œì™€ ë¬´ê´€í•œ ë‚´ìš© ìƒì„± ê°€ëŠ¥

```python
Query: "What is Python?"

Retrieved Doc: "Python is a programming language created in 1991..."

Generated: "Python is a snake found in tropical regions..." âŒ
# Generatorê°€ "Python" = snake ì˜ë¯¸ë¡œ ìƒì„±
```

**í•´ê²° ë°©ì•ˆ**:
- **Constrained Decoding**: ê²€ìƒ‰ ë¬¸ì„œì˜ ë‹¨ì–´ë§Œ ì‚¬ìš©í•˜ë„ë¡ ì œì•½
- **Attention Supervision**: Generatorê°€ ë¬¸ì„œì— ì§‘ì¤‘í•˜ë„ë¡ í•™ìŠµ
- **Fact Verification**: ìƒì„± í›„ ì‚¬ì‹¤ ê²€ì¦ ë‹¨ê³„ ì¶”ê°€

## ğŸš€ í›„ì† ì—°êµ¬ ë° ë°œì „ ë°©í–¥

### 1. FiD (Fusion-in-Decoder, 2021)

**í•µì‹¬ ì•„ì´ë””ì–´**: ì—¬ëŸ¬ ë¬¸ì„œë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì¸ì½”ë”© í›„ ë””ì½”ë”ì—ì„œ ìœµí•©

```python
# RAG
Input: concat(query, doc1)  # 512 tokens max
       concat(query, doc2)
       ...

# FiD
Input: [query + doc1, query + doc2, ..., query + doc_k]
# ê°ê° ë…ë¦½ ì¸ì½”ë”© â†’ ë””ì½”ë”ì—ì„œ cross-attentionìœ¼ë¡œ ìœµí•©
```

**ì¥ì **:
- ë” ë§ì€ ë¬¸ì„œ í™œìš© ê°€ëŠ¥ (K=100)
- ë¬¸ì„œ ê°„ ë…ë¦½ì„± ìœ ì§€ â†’ ê¸´ ë¬¸ì„œ ì²˜ë¦¬ ê°€ëŠ¥

**ì„±ëŠ¥**:
```
NQ Dataset:
- RAG: 44.5% (K=10)
- FiD: 51.4% (K=100) (+6.9%)
```

### 2. RETRO (Retrieval-Enhanced Transformer, 2022)

**í•µì‹¬ ì•„ì´ë””ì–´**: ì‚¬ì „í•™ìŠµ ë‹¨ê³„ë¶€í„° retrieval í†µí•©

```python
# RAG: Fine-tuning only
Pretrain: BART (no retrieval)
Fine-tune: + Retrieval

# RETRO: Pretrain with retrieval
Pretrain: Transformer + Retrieval (2T tokens)
Fine-tune: Same architecture
```

**êµ¬ì¡°**:
- ë§¤ 64 í† í°ë§ˆë‹¤ retrieval ìˆ˜í–‰
- Chunked Cross-Attentionìœ¼ë¡œ íš¨ìœ¨ì  ì²˜ë¦¬
- 7B ëª¨ë¸ì´ 25B ëª¨ë¸ ì„±ëŠ¥ ë‹¬ì„±

### 3. Atlas (Meta, 2022)

**í•µì‹¬ ì•„ì´ë””ì–´**: Few-shot learning + Retrieval

```python
# 5-shot learning with retrieval
Examples = [(Q1, A1), (Q2, A2), ..., (Q5, A5)]

for example in Examples:
    retrieved_docs = retrieve(example.Q)
    # Few-shotì—ì„œë„ retrieval í™œìš©
```

**ì„±ëŠ¥**:
```
NQ (5-shot):
- GPT-3 175B: 29.9%
- Atlas 11B: 42.4% (+12.5%)
```

### 4. Self-RAG (2023)

**í•µì‹¬ ì•„ì´ë””ì–´**: ëª¨ë¸ì´ ìŠ¤ìŠ¤ë¡œ retrieval í•„ìš” ì—¬ë¶€ íŒë‹¨

```python
# Special tokens
[Retrieve]: "I need to retrieve"
[No Retrieve]: "I can answer without retrieval"
[IsRel]: "This document is relevant"
[IsSup]: "This supports my answer"

# Example
Query: "What is 2+2?"
Model: [No Retrieve] 4

Query: "Who created Python?"
Model: [Retrieve] â†’ Search â†’ [IsRel] â†’ [IsSup] Guido van Rossum
```

**ì¥ì **:
- ë¶ˆí•„ìš”í•œ ê²€ìƒ‰ ë°©ì§€ (latency ê°ì†Œ)
- ìƒì„± í’ˆì§ˆ ìì²´ í‰ê°€

### 5. RAG íš¨ìœ¨í™” ì—°êµ¬

#### a) Adaptive Retrieval

```python
# Kë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì ˆ
def adaptive_retrieve(query, confidence_threshold=0.8):
    k = 1
    while k <= 20:
        docs = retrieve(query, k)
        answer, confidence = generate(query, docs)

        if confidence > confidence_threshold:
            return answer
        k += 5

    return answer
```

#### b) Learned Sparse Retrieval

**SPLADE (2021)**:
```python
# Dense: ëª¨ë“  768 ì°¨ì› ì‚¬ìš©
query_vec = [0.1, 0.3, ..., 0.05]  # 768 dims

# Sparse: ì¤‘ìš”í•œ ì°¨ì›ë§Œ
query_vec = [0, 0.9, 0, 0, 0.7, 0, ...]  # ~50 non-zero
# ê²€ìƒ‰ ì†ë„ 10ë°° í–¥ìƒ, ë©”ëª¨ë¦¬ 5ë°° ì ˆê°
```

#### c) Hybrid Search

```python
# BM25 (Lexical) + Dense (Semantic) ì•™ìƒë¸”
def hybrid_search(query, alpha=0.5):
    bm25_scores = bm25_index.search(query)
    dense_scores = faiss_index.search(query_embedding)

    final_scores = alpha * bm25_scores + (1-alpha) * dense_scores
    return final_scores.topk(10)

# NQ: +2.3% over dense-only
```

### 6. Multimodal RAG

**CLIP-RAG (2023)**:
```python
# ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ ê²€ìƒ‰
Query: "Show me pictures of Python's creator"

# Retrieve
image_docs = clip_index.search(query)  # Images of Guido
text_docs = dpr_index.search(query)    # Bio text

# Generate
multimodal_generator(query, image_docs, text_docs)
â†’ Image + Caption
```

## ğŸ¢ ì‹¤ë¬´ ì ìš© ì‚¬ë¡€

### 1. ê³ ê° ì§€ì› ì±—ë´‡

```python
class CustomerSupportRAG:
    def __init__(self, company_docs_path):
        # FAQ, ì œí’ˆ ë§¤ë‰´ì–¼, ê³¼ê±° í‹°ì¼“ì„ ì¸ë±ìŠ¤ë¡œ êµ¬ì¶•
        self.rag = ProductionRAG(company_docs_path)

        # ì‹ ë¢°ë„ ê¸°ë°˜ escalation
        self.confidence_threshold = 0.75

    def handle_query(self, customer_question, customer_id):
        # 1. RAGë¡œ ë‹µë³€ ìƒì„±
        result = self.rag.answer_with_sources(customer_question)

        # 2. ì‹ ë¢°ë„ í‰ê°€
        confidence = self.compute_confidence(result)

        # 3. ë‚®ì€ ì‹ ë¢°ë„ â†’ ì¸ê°„ ìƒë‹´ì› ì—ìŠ¤ì»¬ë ˆì´ì…˜
        if confidence < self.confidence_threshold:
            return {
                "response": "ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”. ìƒë‹´ì›ì„ ì—°ê²°í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
                "escalate": True,
                "agent_context": {
                    "question": customer_question,
                    "attempted_answer": result['answer'],
                    "sources": result['sources']
                }
            }

        # 4. ë†’ì€ ì‹ ë¢°ë„ â†’ ìë™ ì‘ë‹µ
        return {
            "response": result['answer'],
            "escalate": False,
            "sources": [src['title'] for src in result['sources'][:3]],
            "confidence": confidence
        }

    def compute_confidence(self, result):
        # Heuristics
        # 1. ê²€ìƒ‰ ë¬¸ì„œ ì ìˆ˜
        avg_doc_score = np.mean([src['score'] for src in result['sources']])

        # 2. ìƒì„± í™•ë¥  (beam search score)
        gen_score = result.get('generation_score', 0.5)

        # 3. ë‹µë³€ ê¸¸ì´ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸¸ë©´ ë‚®ìŒ)
        answer_len = len(result['answer'].split())
        len_score = 1.0 if 5 < answer_len < 50 else 0.5

        confidence = 0.5 * avg_doc_score + 0.3 * gen_score + 0.2 * len_score
        return confidence

# ì‹¤ì œ ì‚¬ìš©
support_bot = CustomerSupportRAG("./company_docs_index")

response = support_bot.handle_query(
    "How do I reset my password?",
    customer_id="C12345"
)

if response['escalate']:
    # ìƒë‹´ì› UIì— ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬
    route_to_agent(response['agent_context'])
else:
    # ê³ ê°ì—ê²Œ ìë™ ì‘ë‹µ
    send_to_customer(response['response'], response['sources'])
```

**ì‹¤ì œ íš¨ê³¼ (í•œ ìŠ¤íƒ€íŠ¸ì—… ì‚¬ë¡€)**:
```
Before RAG:
- ìë™ í•´ê²°ìœ¨: 35%
- í‰ê·  ì‘ë‹µ ì‹œê°„: 24ë¶„ (ì¸ê°„ ëŒ€ê¸°)
- ê³ ê° ë§Œì¡±ë„: 3.2/5

After RAG:
- ìë™ í•´ê²°ìœ¨: 68% (+94% í–¥ìƒ)
- í‰ê·  ì‘ë‹µ ì‹œê°„: 3ì´ˆ (ì¦‰ì‹œ) / 18ë¶„ (ì—ìŠ¤ì»¬ë ˆì´ì…˜)
- ê³ ê° ë§Œì¡±ë„: 4.1/5
- ìƒë‹´ì› ë¶€ë‹´: -50%
```

### 2. ê¸°ì—… ë‚´ë¶€ ë¬¸ì„œ ê²€ìƒ‰

```python
class EnterpriseRAG:
    def __init__(self, doc_sources):
        """
        doc_sources: {
            "confluence": "./indices/confluence",
            "sharepoint": "./indices/sharepoint",
            "slack": "./indices/slack_history",
            "code": "./indices/github_repos"
        }
        """
        self.retrievers = {
            name: self.load_retriever(path)
            for name, path in doc_sources.items()
        }

        self.generator = self.load_generator()

    def search(self, query, user_permissions, filters=None):
        """
        filters: {
            "sources": ["confluence", "sharepoint"],
            "date_range": ("2024-01-01", "2024-12-31"),
            "departments": ["Engineering", "Product"]
        }
        """
        # 1. ê¶Œí•œ ê¸°ë°˜ ì†ŒìŠ¤ í•„í„°ë§
        allowed_sources = self.filter_by_permissions(
            self.retrievers.keys(),
            user_permissions
        )

        # 2. ê° ì†ŒìŠ¤ì—ì„œ ê²€ìƒ‰
        all_docs = []
        for source in allowed_sources:
            if filters and filters.get("sources"):
                if source not in filters["sources"]:
                    continue

            docs = self.retrievers[source].retrieve(query, k=5)

            # Metadata í•„í„°ë§
            if filters:
                docs = self.apply_filters(docs, filters)

            # ì†ŒìŠ¤ íƒœê¹…
            for doc in docs:
                doc['source'] = source

            all_docs.extend(docs)

        # 3. ì¬ë­í‚¹ (Cross-encoder)
        reranked_docs = self.rerank(query, all_docs, top_k=10)

        # 4. ë‹µë³€ ìƒì„±
        answer = self.generator.generate(query, reranked_docs)

        # 5. ì ‘ê·¼ ê¶Œí•œ ì²´í¬
        answer['sources_with_access'] = [
            {
                **doc,
                "can_access": self.check_access(doc, user_permissions)
            }
            for doc in reranked_docs
        ]

        return answer

    def filter_by_permissions(self, sources, user_permissions):
        # RBAC (Role-Based Access Control)
        allowed = []
        for source in sources:
            if user_permissions.get(f"read_{source}", False):
                allowed.append(source)
        return allowed

    def apply_filters(self, docs, filters):
        filtered = docs

        # ë‚ ì§œ í•„í„°
        if filters.get("date_range"):
            start, end = filters["date_range"]
            filtered = [
                doc for doc in filtered
                if start <= doc['metadata']['date'] <= end
            ]

        # ë¶€ì„œ í•„í„°
        if filters.get("departments"):
            filtered = [
                doc for doc in filtered
                if doc['metadata'].get('department') in filters["departments"]
            ]

        return filtered

    def rerank(self, query, docs, top_k=10):
        # Cross-encoderë¡œ ì •ë°€ ì¬ë­í‚¹
        from sentence_transformers import CrossEncoder

        reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

        pairs = [[query, doc['text']] for doc in docs]
        scores = reranker.predict(pairs)

        # ì ìˆ˜ ìˆœ ì •ë ¬
        ranked_indices = np.argsort(scores)[::-1][:top_k]
        return [docs[i] for i in ranked_indices]

# ì‚¬ìš© ì˜ˆì‹œ
enterprise_rag = EnterpriseRAG({
    "confluence": "./indices/confluence",
    "sharepoint": "./indices/sharepoint",
    "slack": "./indices/slack",
    "github": "./indices/github"
})

user = {
    "id": "john@company.com",
    "permissions": {
        "read_confluence": True,
        "read_sharepoint": True,
        "read_slack": False,  # No access
        "read_github": True
    }
}

result = enterprise_rag.search(
    query="What is our Q4 2024 revenue target?",
    user_permissions=user["permissions"],
    filters={
        "sources": ["confluence", "sharepoint"],
        "date_range": ("2024-01-01", "2024-12-31"),
        "departments": ["Finance", "Executive"]
    }
)

print(f"Answer: {result['answer']}")
print(f"Sources:")
for src in result['sources_with_access']:
    access_icon = "ğŸ”“" if src['can_access'] else "ğŸ”’"
    print(f"  {access_icon} {src['title']} ({src['source']})")
```

### 3. ì˜ë£Œ Q&A ì‹œìŠ¤í…œ

```python
class MedicalRAG:
    def __init__(self):
        # PubMed ë…¼ë¬¸, ì˜í•™ êµê³¼ì„œ, ì„ìƒ ê°€ì´ë“œë¼ì¸
        self.rag = ProductionRAG("./medical_literature_index")

        # Medical NER (Named Entity Recognition)
        from transformers import pipeline
        self.ner = pipeline(
            "ner",
            model="alvaroalon2/biobert_diseases_ner"
        )

    def answer_medical_query(self, question, user_type="patient"):
        # 1. ì˜í•™ ìš©ì–´ ì¶”ì¶œ
        entities = self.ner(question)
        diseases = [e['word'] for e in entities if e['entity'] == 'Disease']

        # 2. RAGë¡œ ë‹µë³€ ìƒì„±
        result = self.rag.answer_with_sources(question, num_docs=10)

        # 3. ì¶œì²˜ í‰ê°€ (Evidence Level)
        evidence_level = self.assess_evidence(result['sources'])

        # 4. ì‚¬ìš©ì ìœ í˜•ë³„ ë‹µë³€ ì¡°ì •
        if user_type == "patient":
            answer = self.simplify_medical_terms(result['answer'])
        elif user_type == "doctor":
            answer = result['answer']  # ì „ë¬¸ ìš©ì–´ ìœ ì§€

        # 5. ì¸ìš© í˜•ì‹ ìƒì„± (APA)
        citations = self.format_citations(result['sources'])

        return {
            "answer": answer,
            "evidence_level": evidence_level,
            "citations": citations,
            "detected_conditions": diseases,
            "disclaimer": self.get_disclaimer()
        }

    def assess_evidence(self, sources):
        """
        Evidence Level (ì˜í•™ ê·¼ê±° ë“±ê¸‰):
        - Level 1: Systematic Review / Meta-analysis
        - Level 2: Randomized Controlled Trial (RCT)
        - Level 3: Cohort Study
        - Level 4: Case-Control Study
        - Level 5: Expert Opinion
        """
        levels = []
        for src in sources:
            # ë©”íƒ€ë°ì´í„°ì—ì„œ ì—°êµ¬ íƒ€ì… ì¶”ì¶œ
            study_type = src.get('metadata', {}).get('study_type', 'unknown')

            if 'meta-analysis' in study_type.lower():
                levels.append(1)
            elif 'rct' in study_type.lower():
                levels.append(2)
            elif 'cohort' in study_type.lower():
                levels.append(3)
            elif 'case-control' in study_type.lower():
                levels.append(4)
            else:
                levels.append(5)

        # ìµœê³  ê·¼ê±° ë“±ê¸‰ ë°˜í™˜
        return min(levels) if levels else 5

    def simplify_medical_terms(self, text):
        # ì˜í•™ ìš©ì–´ â†’ ì¼ë°˜ ìš©ì–´ ë³€í™˜
        replacements = {
            "myocardial infarction": "ì‹¬ì¥ë§ˆë¹„",
            "cerebrovascular accident": "ë‡Œì¡¸ì¤‘",
            "hypertension": "ê³ í˜ˆì••",
            # ... ìˆ˜ë°± ê°œ ë§¤í•‘
        }

        for medical, simple in replacements.items():
            text = text.replace(medical, f"{simple}({medical})")

        return text

    def format_citations(self, sources):
        # APA í˜•ì‹ ì¸ìš©
        citations = []
        for i, src in enumerate(sources[:5], 1):
            meta = src.get('metadata', {})
            citation = (
                f"{i}. {meta.get('authors', 'Unknown')} "
                f"({meta.get('year', 'n.d.')}). "
                f"{meta.get('title', 'Untitled')}. "
                f"{meta.get('journal', 'Unknown Journal')}. "
                f"DOI: {meta.get('doi', 'N/A')}"
            )
            citations.append(citation)
        return citations

    def get_disclaimer(self):
        return (
            "âš ï¸ This information is for educational purposes only and "
            "should not replace professional medical advice. "
            "Please consult a qualified healthcare provider for "
            "diagnosis and treatment."
        )

# ì‚¬ìš© ì˜ˆì‹œ
medical_rag = MedicalRAG()

# í™˜ììš©
patient_result = medical_rag.answer_medical_query(
    question="What are the symptoms of myocardial infarction?",
    user_type="patient"
)

print(f"Answer: {patient_result['answer']}")
print(f"Evidence Level: {patient_result['evidence_level']}/5")
print(f"Citations:")
for cite in patient_result['citations']:
    print(f"  {cite}")
print(f"\n{patient_result['disclaimer']}")

# ì˜ì‚¬ìš©
doctor_result = medical_rag.answer_medical_query(
    question="What is the recommended antiplatelet therapy for NSTEMI?",
    user_type="doctor"
)
```

**ì‹¤ì œ íš¨ê³¼ (ë³‘ì› ì‚¬ë¡€)**:
```
Before:
- ì˜ì‚¬ê°€ ê°€ì´ë“œë¼ì¸ ì°¾ëŠ” ì‹œê°„: í‰ê·  15ë¶„
- ìµœì‹  ì—°êµ¬ ë°˜ì˜: 6ê°œì›” ì§€ì—°

After RAG:
- ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰: 10ì´ˆ
- ìµœì‹  ì—°êµ¬: ì£¼ 1íšŒ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
- ì§„ë£Œ íš¨ìœ¨: +20%
- ê·¼ê±° ê¸°ë°˜ ì˜í•™ ì‹¤ì²œ: í–¥ìƒ
```

## ğŸ”‘ í•µì‹¬ ìš”ì•½

### RAGì˜ í•µì‹¬ ê°€ì¹˜

**1. Hybrid Memory Architecture**
```
Parametric Memory (LLM)     +     Non-Parametric Memory (Documents)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
- ì–¸ì–´ ì´í•´                      - ì‚¬ì‹¤ ì§€ì‹
- ì¶”ë¡  ëŠ¥ë ¥                      - ìµœì‹  ì •ë³´
- ì¼ë°˜í™”                         - ì¶œì²˜ ì¶”ì 
- ê³ ì • (í•™ìŠµ ì‹œì )              - ì—…ë°ì´íŠ¸ ìš©ì´
```

**2. í•µì‹¬ ì¥ì **

| íŠ¹ì„± | Closed-Book LLM | RAG | ê°œì„  |
|------|----------------|-----|------|
| Hallucination | ë†’ìŒ | ë‚®ìŒ | âœ… ê·¼ê±° ê¸°ë°˜ ìƒì„± |
| ì§€ì‹ ì—…ë°ì´íŠ¸ | ì¬í•™ìŠµ í•„ìš” ($$$) | ì¸ë±ìŠ¤ êµì²´ | âœ… ë¹„ìš© 1/10000 |
| ì¶œì²˜ ì¶”ì  | ë¶ˆê°€ëŠ¥ | ê°€ëŠ¥ | âœ… ì‹ ë¢°ì„± í–¥ìƒ |
| íŒŒë¼ë¯¸í„° íš¨ìœ¨ | 11B | 626M | âœ… 17ë°° ì‘ìŒ |
| ì„±ëŠ¥ (NQ) | 34.5% | 44.5% | âœ… +10% |

**3. ì‹¤ë¬´ ì ìš©**

âœ… **ChatGPT Enterprise**: "Browse with Bing" ê¸°ëŠ¥
âœ… **Microsoft Copilot**: SharePoint/OneDrive í†µí•©
âœ… **Notion AI**: ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ë¬¸ì„œ ê²€ìƒ‰
âœ… **Perplexity AI**: ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ + ìƒì„±
âœ… **ê¸°ì—… Q&A**: ì‚¬ë‚´ ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ

### êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

**Phase 1: í”„ë¡œí† íƒ€ì… (1-2ì£¼)**
```python
â˜ HuggingFace RAG ëª¨ë¸ ë¡œë“œ
â˜ ê¸°ë³¸ QA í…ŒìŠ¤íŠ¸
â˜ ìƒ˜í”Œ ë¬¸ì„œ (100-1000ê°œ)ë¡œ ì¸ë±ìŠ¤ êµ¬ì¶•
â˜ Accuracy/Latency ì¸¡ì •
```

**Phase 2: ì»¤ìŠ¤í…€ ë°ì´í„° (2-4ì£¼)**
```python
â˜ ìì²´ ë¬¸ì„œ ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬
â˜ DPRë¡œ ë¬¸ì„œ ë²¡í„°í™”
â˜ FAISS ì¸ë±ìŠ¤ êµ¬ì¶• (ìˆ˜ë§Œ~ìˆ˜ì‹­ë§Œ ë¬¸ì„œ)
â˜ Fine-tuning (optional)
```

**Phase 3: Production (4-8ì£¼)**
```python
â˜ FAISS GPU ìµœì í™”
â˜ ì¸ë±ìŠ¤ ì••ì¶• (PQ/IVF)
â˜ ë°°ì¹˜ ì¶”ë¡  íŒŒì´í”„ë¼ì¸
â˜ ëª¨ë‹ˆí„°ë§ (Latency, Accuracy, Cache Hit Rate)
â˜ A/B í…ŒìŠ¤íŠ¸
```

**Phase 4: ê³ ë„í™” (ì§€ì†)**
```python
â˜ Hybrid Search (BM25 + Dense)
â˜ Re-ranking (Cross-encoder)
â˜ Self-RAG (adaptive retrieval)
â˜ Multi-hop reasoning
```

## ğŸ“– ì°¸ê³  ìë£Œ

### ë…¼ë¬¸
- **ì› ë…¼ë¬¸**: [RAG (Lewis et al., NeurIPS 2020)](https://arxiv.org/abs/2005.11401)
- **DPR**: [Dense Passage Retrieval (Karpukhin et al., EMNLP 2020)](https://arxiv.org/abs/2004.04906)
- **REALM**: [Retrieval-Augmented Language Model Pre-Training (Guu et al., ICML 2020)](https://arxiv.org/abs/2002.08909)
- **FiD**: [Fusion-in-Decoder (Izacard & Grave, EACL 2021)](https://arxiv.org/abs/2007.01282)
- **RETRO**: [Improving LMs by Retrieving from Trillions of Tokens (Borgeaud et al., 2022)](https://arxiv.org/abs/2112.04426)
- **Self-RAG**: [Self-Reflective RAG (Asai et al., 2023)](https://arxiv.org/abs/2310.11511)

### ê³µì‹ êµ¬í˜„
- **HuggingFace Transformers**: [RAG Documentation](https://huggingface.co/docs/transformers/model_doc/rag)
- **Facebook Research**: [Original Implementation](https://github.com/facebookresearch/RAG)
- **FAISS**: [Facebook AI Similarity Search](https://github.com/facebookresearch/faiss)

### í”„ë ˆì„ì›Œí¬ ë° ë„êµ¬
- **LangChain**: [RAG Chains](https://python.langchain.com/docs/use_cases/question_answering/)
- **LlamaIndex**: [Data Framework for LLMs](https://www.llamaindex.ai/)
- **Haystack**: [NLP Framework by deepset](https://haystack.deepset.ai/)
- **Weaviate**: [Vector Database](https://weaviate.io/)
- **Pinecone**: [Managed Vector Database](https://www.pinecone.io/)

### íŠœí† ë¦¬ì–¼
- **HuggingFace Course**: [RAG Tutorial](https://huggingface.co/learn/nlp-course/chapter7/6)
- **Google Colab**: [RAG Notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/question_answering.ipynb)

### ë¸”ë¡œê·¸ ë° ë¦¬ì†ŒìŠ¤
- **Anthropic**: [RAG in Production](https://www.anthropic.com/index/retrieval-augmented-generation)
- **Pinecone Blog**: [RAG Guide](https://www.pinecone.io/learn/retrieval-augmented-generation/)
- **LangChain Blog**: [Advanced RAG Techniques](https://blog.langchain.dev/tag/rag/)

---

**ì´ ë…¼ë¬¸ì€ í˜„ëŒ€ LLM ì‹œìŠ¤í…œì˜ ê·¼ê°„ì´ ë˜ëŠ” RAG ì•„í‚¤í…ì²˜ë¥¼ ì œì‹œí•˜ì—¬, ì‹¤ë¬´ì—ì„œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” AI ì„œë¹„ìŠ¤ êµ¬ì¶•ì˜ í•µì‹¬ ê¸°ìˆ ë¡œ ìë¦¬ì¡ì•˜ìŠµë‹ˆë‹¤.**
