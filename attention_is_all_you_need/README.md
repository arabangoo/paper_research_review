# Attention Is All You Need - ë…¼ë¬¸ ë¦¬ë·°

## ğŸ“Œ ë…¼ë¬¸ ì†Œê°œ

**ì œëª©**: Attention Is All You Need  
**ì €ì**: Ashish Vaswani, Noam Shazeer, Niki Parmar ì™¸ (Google Brain/Research)  
**ë°œí‘œ**: NIPS 2017  
**arXiv**: https://arxiv.org/abs/1706.03762

## ğŸ¯ í•µì‹¬ ê°€ì¹˜

ì´ ë…¼ë¬¸ì€ ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì˜ íŒ¨ëŸ¬ë‹¤ì„ì„ ì™„ì „íˆ ë°”ê¾¼ í˜ì‹ ì ì¸ ì—°êµ¬ì…ë‹ˆë‹¤.    
RNNê³¼ CNN ì—†ì´ ì˜¤ì§ **Attention ë©”ì»¤ë‹ˆì¦˜**ë§Œìœ¼ë¡œ êµ¬ì„±ëœ **Transformer ì•„í‚¤í…ì²˜**ë¥¼ ì œì•ˆí•˜ì—¬ ë§ì€ ì„±ê³¼ë¥¼ ì´ë£¨ì—ˆìŠµë‹ˆë‹¤.       

- ê¸°ì¡´ Seq2Seq ëª¨ë¸ë“¤ì˜ ìˆœì°¨ ì²˜ë¦¬ ë³‘ëª© í˜„ìƒ í•´ê²°
- ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í†µí•œ í•™ìŠµ ì†ë„ ëŒ€í­ í–¥ìƒ
- WMT 2014 ì˜ì–´-ë…ì¼ì–´ ë²ˆì—­ì—ì„œ BLEU 28.4 ë‹¬ì„± (ë‹¹ì‹œ SOTA)
- BERT, GPT ë“± í˜„ëŒ€ LLMì˜ ê¸°ë°˜ì´ ë˜ëŠ” ì•„í‚¤í…ì²˜ ì œì‹œ

## ğŸ”™ ì—°êµ¬ ë°°ê²½ ë° ë™ê¸°

### ê¸°ì¡´ ëª¨ë¸ì˜ í•œê³„

**1. RNN/LSTMì˜ ê·¼ë³¸ì ì¸ ë¬¸ì œ**
- **ìˆœì°¨ ì²˜ë¦¬ ì œì•½**: ì‹œê°„ tì˜ ì¶œë ¥ì„ ê³„ì‚°í•˜ë ¤ë©´ t-1ì˜ hidden stateê°€ í•„ìš” â†’ ë³‘ë ¬í™” ë¶ˆê°€
- **Long-term Dependency ë¬¸ì œ**: ê¸´ ì‹œí€€ìŠ¤ì—ì„œ ì´ˆê¸° ì •ë³´ê°€ ì†ì‹¤ë˜ëŠ” vanishing gradient ë¬¸ì œ
- **í•™ìŠµ ì‹œê°„**: ê¸´ ë¬¸ì¥ì¼ìˆ˜ë¡ í•™ìŠµ ì‹œê°„ì´ ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€
- **ë©”ëª¨ë¦¬ ë³‘ëª©**: ê° timestepì˜ hidden stateë¥¼ ì €ì¥í•´ì•¼ í•˜ë¯€ë¡œ ë©”ëª¨ë¦¬ ë¹„íš¨ìœ¨

**2. CNN ê¸°ë°˜ Seq2Seqì˜ í•œê³„**
- **ByteNet, ConvS2S** ê°™ì€ ëª¨ë¸ë“¤ì´ ë³‘ë ¬í™”ë¥¼ ì‹œë„í–ˆì§€ë§Œ:
  - ê¸´ ê±°ë¦¬ dependency í•™ìŠµì— ì—¬ì „íˆ ì œì•½ (O(log_k(n)) ê²½ë¡œ ê¸¸ì´)
  - Receptive field í™•ì¥ì„ ìœ„í•´ ë§ì€ ë ˆì´ì–´ í•„ìš”
  - ê³„ì‚° ë³µì¡ë„: O(kÂ·nÂ·dÂ²) - ì»¤ë„ í¬ê¸° kì— ë¹„ë¡€

**3. Attention Mechanismì˜ ë¶€ìƒ**
- **Bahdanau et al. (2015)**: Seq2Seqì— Attention ë„ì…
- **ë¬¸ì œì **: RNN/LSTMê³¼ í•¨ê»˜ ì‚¬ìš© â†’ ì—¬ì „íˆ ìˆœì°¨ ì²˜ë¦¬ í•„ìš”
- **í•µì‹¬ ì§ˆë¬¸**: "Attentionë§Œìœ¼ë¡œ ì¶©ë¶„í•˜ì§€ ì•Šì„ê¹Œ?" â†’ ì´ ë…¼ë¬¸ì˜ ì¶œë°œì 

### ì´ ë…¼ë¬¸ì´ í•´ê²°í•˜ê³ ì í•œ í•µì‹¬ ê³¼ì œ

1. **ë³‘ë ¬í™”**: ìˆœì°¨ ì²˜ë¦¬ ì—†ì´ ëª¨ë“  ìœ„ì¹˜ë¥¼ ë™ì‹œì— ê³„ì‚°
2. **íš¨ìœ¨ì„±**: ì ì€ ê³„ì‚°ëŸ‰ìœ¼ë¡œ ê¸´ dependency í•™ìŠµ
3. **ì„±ëŠ¥**: ê¸°ì¡´ SOTA ëª¨ë¸ ëŠ¥ê°€
4. **ì¼ë°˜í™”**: ë‹¤ì–‘í•œ NLP íƒœìŠ¤í¬ì— ì ìš© ê°€ëŠ¥í•œ ë²”ìš© ì•„í‚¤í…ì²˜

## ğŸ—ï¸ ëª¨ë¸ ì•„í‚¤í…ì²˜

### 1. ì „ì²´ êµ¬ì¡°
TransformerëŠ” Encoder-Decoder êµ¬ì¡°ë¥¼ ë”°ë¥´ì§€ë§Œ, RNN/LSTMì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

```
Inputs â†’ Input Embedding + Positional Encoding
        â†“
    [Encoder Stack (N=6)]
        â†“
    [Decoder Stack (N=6)]
        â†“
    Linear + Softmax â†’ Output Probabilities
```

### 2. Encoder
- **N=6ê°œ**ì˜ ë™ì¼í•œ ë ˆì´ì–´ë¥¼ ìŒ“ì€ êµ¬ì¡°
- ê° ë ˆì´ì–´ëŠ” 2ê°œì˜ Sub-layerë¡œ êµ¬ì„±:
  - Multi-Head Self-Attention
  - Position-wise Feed-Forward Network
- ê° Sub-layer í›„ì— **Residual Connection + Layer Normalization** ì ìš©
- ì¶œë ¥ ì°¨ì›: **d_model = 512**

### 3. Decoder
- **N=6ê°œ**ì˜ ë™ì¼í•œ ë ˆì´ì–´ë¥¼ ìŒ“ì€ êµ¬ì¡°
- ê° ë ˆì´ì–´ëŠ” 3ê°œì˜ Sub-layerë¡œ êµ¬ì„±:
  - **Masked** Multi-Head Self-Attention (ë¯¸ë˜ í† í° ì°¸ì¡° ë°©ì§€)
  - Multi-Head Encoder-Decoder Attention
  - Position-wise Feed-Forward Network
- ë™ì¼í•˜ê²Œ **Residual Connection + Layer Normalization** ì ìš©## ğŸ” í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜ í•´ì„¤

### 1. Scaled Dot-Product Attention

**ìˆ˜ì‹**:
```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) V
```

**ë™ì‘ ì›ë¦¬**:
1. Query(Q)ì™€ Key(K)ì˜ ë‚´ì ìœ¼ë¡œ ìœ ì‚¬ë„ ê³„ì‚°
2. âˆšd_kë¡œ ìŠ¤ì¼€ì¼ë§ (gradient ì•ˆì •í™”)
3. Softmaxë¡œ í™•ë¥  ë¶„í¬ ë³€í™˜
4. Value(V)ì— ê°€ì¤‘ì¹˜ë¥¼ ê³±í•´ ìµœì¢… ì¶œë ¥

**ì™œ ìŠ¤ì¼€ì¼ë§ì´ í•„ìš”í•œê°€?**
- d_kê°€ í´ ë•Œ ë‚´ì  ê°’ì´ ë„ˆë¬´ ì»¤ì ¸ softmaxê°€ ê·¹ë‹¨ì ìœ¼ë¡œ ì‘ì€ gradientë¥¼ ê°€ì§€ê²Œ ë¨
- ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ âˆšd_kë¡œ ë‚˜ëˆ ì¤Œ

### 2. Multi-Head Attention

**ê°œë…**:
- ë‹¨ì¼ Attention ëŒ€ì‹  **h=8ê°œ**ì˜ ë³‘ë ¬ Attention Head ì‚¬ìš©
- ê° HeadëŠ” ì„œë¡œ ë‹¤ë¥¸ representation subspaceë¥¼ í•™ìŠµ
- ìµœì¢…ì ìœ¼ë¡œ Concatenate í›„ Linear ë³€í™˜

**ìˆ˜ì‹**:
```
MultiHead(Q,K,V) = Concat(head_1, ..., head_h)W^O
where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
```

**ì¥ì **:
- ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ë¬¸ë§¥ ì •ë³´ í¬ì°©
- ë³‘ë ¬ ì²˜ë¦¬ë¡œ ê³„ì‚° íš¨ìœ¨ì„± ìœ ì§€ (ì´ ê³„ì‚°ëŸ‰ì€ single-headì™€ ìœ ì‚¬)

### 3. Attentionì˜ 3ê°€ì§€ í™œìš©

**(1) Encoder Self-Attention**
- Q, K, V ëª¨ë‘ ì´ì „ Encoder ë ˆì´ì–´ ì¶œë ¥ â†’ ì…ë ¥ ë¬¸ì¥ ë‚´ ëª¨ë“  ë‹¨ì–´ ê°„ ê´€ê³„ í•™ìŠµ
- ì˜ˆì‹œ: "I love her"
  - "I"ê°€ Queryì¼ ë•Œ â†’ "I", "love", "her" ëª¨ë‘ì™€ ê´€ê³„ ê³„ì‚°

**(2) Decoder Self-Attention (Masked)**
- Q, K, V ëª¨ë‘ ì´ì „ Decoder ë ˆì´ì–´ ì¶œë ¥ â†’ ë‹¨, ë¯¸ë˜ ìœ„ì¹˜ ì°¸ì¡° ë°©ì§€ (Masking)
- **Masking ì´ìœ **: Auto-regressive ì†ì„± ìœ ì§€ (ë¯¸ë˜ ì •ë³´ ëˆ„ì„¤ ë°©ì§€)

**(3) Encoder-Decoder Attention**
- Q: Decoder ì¶œë ¥, K, V: Encoder ìµœì¢… ì¶œë ¥ â†’ ì…ë ¥ ë¬¸ì¥ê³¼ ì¶œë ¥ ë¬¸ì¥ ê°„ ê´€ê³„ í•™ìŠµ

### 4. Positional Encoding

**ë¬¸ì œ**: TransformerëŠ” ìˆœì°¨ ì²˜ë¦¬ê°€ ì—†ì–´ ìœ„ì¹˜ ì •ë³´ ë¶€ì¬  
**í•´ê²°**: ì‚¬ì¸/ì½”ì‚¬ì¸ í•¨ìˆ˜ë¡œ ìœ„ì¹˜ ì •ë³´ ì¸ì½”ë”©

**ìˆ˜ì‹**:
```
PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

**íŠ¹ì§•**:
- ê° ìœ„ì¹˜ë§ˆë‹¤ ê³ ìœ í•œ ë²¡í„° ìƒì„±
- ìƒëŒ€ì  ìœ„ì¹˜ ê´€ê³„ í•™ìŠµ ê°€ëŠ¥
- í•™ìŠµ ë°ì´í„°ë³´ë‹¤ ê¸´ ì‹œí€€ìŠ¤ì—ë„ ëŒ€ì‘ ê°€ëŠ¥

### 5. Position-wise Feed-Forward Network

**êµ¬ì¡°**:
```python
FFN(x) = max(0, xW_1 + b_1)W_2 + b_2
```
- 2ê°œì˜ Linear Layer + ReLU activation
- ì…ì¶œë ¥ ì°¨ì›: **d_model = 512**
- ì¤‘ê°„ ì°¨ì›: **d_ff = 2048**
- ê° ìœ„ì¹˜ë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ ì ìš©

### 6. Residual Connectionê³¼ Layer Normalization

**ì™œ í•„ìš”í•œê°€?**
- TransformerëŠ” 6ê°œì˜ ë ˆì´ì–´ë¥¼ ìŒ“ì€ ê¹Šì€ ë„¤íŠ¸ì›Œí¬
- ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì˜ ê³ ì§ˆì  ë¬¸ì œ: gradient vanishing/exploding

**Residual Connection (Skip Connection)**
```python
# ê° Sub-layerì˜ ì¶œë ¥
output = LayerNorm(x + Sublayer(x))
```

**ë™ì‘ ì›ë¦¬**:
1. Sublayer(x): Attention ë˜ëŠ” FFN ê³„ì‚°
2. x + Sublayer(x): ì…ë ¥ì„ ì§ì ‘ ë”í•¨ (residual)
3. LayerNorm: ì •ê·œí™”

**íš¨ê³¼**:
- **Gradient Flow ê°œì„ **: Backprop ì‹œ gradientê°€ residual pathë¥¼ í†µí•´ ì§ì ‘ ì „ë‹¬
- **í•™ìŠµ ì•ˆì •í™”**: ì´ˆê¸° ë ˆì´ì–´ì˜ í•™ìŠµì´ ìš©ì´
- **Identity Mapping**: í•„ìš”ì‹œ Sublayerê°€ 0ì„ í•™ìŠµí•˜ì—¬ ì…ë ¥ì„ ê·¸ëŒ€ë¡œ í†µê³¼ ê°€ëŠ¥

**Layer Normalization**
```python
# ê° ìƒ˜í”Œ, ê° ìœ„ì¹˜ë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ ì •ê·œí™”
mean = x.mean(dim=-1, keepdim=True)
std = x.std(dim=-1, keepdim=True)
LayerNorm(x) = Î³ * (x - mean) / (std + Îµ) + Î²
```

**Batch Norm vs Layer Norm**:
| êµ¬ë¶„ | Batch Normalization | Layer Normalization |
|------|---------------------|---------------------|
| ì •ê·œí™” ì¶• | Batch ì°¨ì› | Feature ì°¨ì› |
| ì‹œí€€ìŠ¤ ì²˜ë¦¬ | ê¸¸ì´ê°€ ë‹¤ë¥´ë©´ ë¬¸ì œ | ê° ìƒ˜í”Œ ë…ë¦½ì  ì²˜ë¦¬ |
| ì¶”ë¡  ì‹œ | Running statistics í•„ìš” | ì¶”ê°€ í†µê³„ ë¶ˆí•„ìš” |
| Transformer | âŒ ë¶€ì í•© | âœ… ì í•© |

**ì‹¤ì œ êµ¬í˜„**:
```python
class SublayerConnection(nn.Module):
    """Residual connection + Layer Normalization"""
    def __init__(self, size, dropout):
        super().__init__()
        self.norm = nn.LayerNorm(size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, sublayer):
        # Post-LN: sublayer í›„ normalization
        return x + self.dropout(sublayer(self.norm(x)))
```

**Pre-LN vs Post-LN**:
- **ë…¼ë¬¸ (Post-LN)**: LayerNorm(x + Sublayer(x))
- **í˜„ëŒ€ êµ¬í˜„ (Pre-LN)**: x + Sublayer(LayerNorm(x))
  - Pre-LNì´ í•™ìŠµ ì•ˆì •ì„± ë©´ì—ì„œ ë” ìš°ìˆ˜ (GPT-2/3, BERT ë“±ì—ì„œ ì±„íƒ)## âš¡ ì£¼ìš” ì¥ì 

### 1. ê³„ì‚° ë³µì¡ë„ ë¹„êµ

| Layer Type | Complexity per Layer | Sequential Operations | Max Path Length |
|------------|---------------------|----------------------|-----------------|
| Self-Attention | O(nÂ²Â·d) | O(1) | O(1) |
| Recurrent | O(nÂ·dÂ²) | O(n) | O(n) |
| Convolutional | O(kÂ·nÂ·dÂ²) | O(1) | O(log_k(n)) |

**Self-Attentionì˜ ìš°ìœ„**:
- **ë³‘ë ¬ ì²˜ë¦¬**: ìˆœì°¨ ì—°ì‚° O(1)
- **Long-range dependency**: ìµœëŒ€ ê²½ë¡œ ê¸¸ì´ O(1)
- **ê³„ì‚° íš¨ìœ¨**: ëŒ€ë¶€ë¶„ì˜ ê²½ìš° n < dì´ë¯€ë¡œ RNNë³´ë‹¤ ë¹ ë¦„

**ìƒì„¸ ë³µì¡ë„ ë¶„ì„**

**ì‹œê°„ ë³µì¡ë„**:
```
Self-Attention í•œ ì¸µì˜ ê³„ì‚°ëŸ‰:
1. Q, K, V ê³„ì‚°: 3 Ã— (n Ã— d Ã— d) = O(nÂ·dÂ²)
2. Attention Score: (n Ã— d) Ã— (d Ã— n) = O(nÂ²Â·d)
3. Attention Ã— V: (n Ã— n) Ã— (n Ã— d) = O(nÂ²Â·d)
â†’ ì´: O(nÂ²Â·d + nÂ·dÂ²)
â†’ n < dì¼ ë•Œ: O(nÂ·dÂ²) (RNNê³¼ ë™ì¼)
â†’ n > dì¼ ë•Œ: O(nÂ²Â·d) (ë³‘ëª©)
```

**ì‹¤ì œ ì˜ˆì‹œ (d_model=512)**:
```python
# ì§§ì€ ë¬¸ì¥ (n=50)
Self-Attention: 50Â² Ã— 512 = 1.28M ops
RNN: 50 Ã— 512Â² = 13.1M ops
â†’ Self-Attentionì´ 10ë°° ë¹ ë¦„

# ê¸´ ë¬¸ì„œ (n=2048)
Self-Attention: 2048Â² Ã— 512 = 2.15B ops
RNN: 2048 Ã— 512Â² = 537M ops
â†’ RNNì´ 4ë°° ë¹ ë¦„ (í•˜ì§€ë§Œ ìˆœì°¨ ì²˜ë¦¬ í•„ìš”)
```

**ê³µê°„ ë³µì¡ë„ (ë©”ëª¨ë¦¬)**:
```
1. Attention Matrix: O(nÂ² Ã— h)
   - n=512, h=8: 512Â² Ã— 8 = 2.1M ìš”ì†Œ
   - n=2048, h=8: 2048Â² Ã— 8 = 33.6M ìš”ì†Œ (16ë°° ì¦ê°€!)

2. Key-Value Cache (ì¶”ë¡  ì‹œ):
   - ê° ë ˆì´ì–´ë§ˆë‹¤: O(n Ã— d Ã— 2)
   - 6ê°œ ë ˆì´ì–´: O(6 Ã— n Ã— 1024) = O(n Ã— 6K)
```

**ì²˜ë¦¬ëŸ‰(Throughput) vs ì§€ì—°ì‹œê°„(Latency) íŠ¸ë ˆì´ë“œì˜¤í”„**:
- **í•™ìŠµ**: Transformer ì••ë„ì  ìš°ìœ„ (ë³‘ë ¬í™”)
- **ì¶”ë¡  (ì§§ì€ ì‹œí€€ìŠ¤)**: Transformer ìš°ì„¸
- **ì¶”ë¡  (ê¸´ ì‹œí€€ìŠ¤)**: RNNì´ ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
- **ìŠ¤íŠ¸ë¦¬ë°**: RNN ìœ ë¦¬ (í† í°ë³„ ìˆœì°¨ ìƒì„±)

### 2. ë³‘ë ¬í™” ê°€ëŠ¥
- **RNN**: ì´ì „ hidden state í•„ìš” â†’ ìˆœì°¨ ì²˜ë¦¬ í•„ìˆ˜
- **Transformer**: ëª¨ë“  ìœ„ì¹˜ ë™ì‹œ ê³„ì‚° â†’ GPU í™œìš© ê·¹ëŒ€í™”

### 3. Long-range Dependency í•™ìŠµ
- **RNN**: ê¸´ ë¬¸ì¥ì—ì„œ ì •ë³´ ì†ì‹¤ (vanishing gradient)
- **Transformer**: ëª¨ë“  ìœ„ì¹˜ ê°„ ì§ì ‘ ì—°ê²° (O(1) path)

## ğŸ“Š ì‹¤í—˜ ê²°ê³¼

### 1. ê¸°ê³„ ë²ˆì—­ ì„±ëŠ¥ (WMT 2014)

| Model | EN-DE BLEU | EN-FR BLEU | Training Cost |
|-------|------------|------------|---------------|
| GNMT + RL | 24.6 | 39.92 | 2.3 Ã— 10Â¹â¹ FLOPs |
| ConvS2S | 25.16 | 40.46 | 1.5 Ã— 10Â²â° FLOPs |
| Transformer (base) | 27.3 | 38.1 | 3.3 Ã— 10Â¹â¸ FLOPs |
| Transformer (big) | **28.4** | **41.8** | 2.3 Ã— 10Â¹â¹ FLOPs |

**í•™ìŠµ í™˜ê²½**:
- Base model: 8 Ã— P100 GPU, 12ì‹œê°„
- Big model: 8 Ã— P100 GPU, 3.5ì¼

### 2. í•˜ì´í¼íŒŒë¼ë¯¸í„°

```python
# Base Model
N = 6                # Encoder/Decoder layers
d_model = 512        # Hidden dimension
d_ff = 2048          # FFN inner dimension
h = 8                # Attention heads
d_k = d_v = 64       # Key/Value dimension (d_model/h)
P_drop = 0.1         # Dropout rate

# Big Model
d_model = 1024
d_ff = 4096
h = 16
P_drop = 0.3
```

### 3. í•™ìŠµ ê¸°ë²• ë° ì •ê·œí™”

**Optimizer: Adam**
```python
# ë…¼ë¬¸ì—ì„œ ì‚¬ìš©í•œ ì„¤ì •
beta1 = 0.9
beta2 = 0.98
epsilon = 1e-9
```

**Learning Rate Scheduler (í•µì‹¬!)**
```python
lrate = d_model^(-0.5) * min(step_num^(-0.5), step_num * warmup_steps^(-1.5))
```

**Warmup ì „ëµ**:
- **warmup_steps = 4000**
- ì´ˆê¸° 4000 step ë™ì•ˆ learning rateë¥¼ ì„ í˜•ìœ¼ë¡œ ì¦ê°€
- ì´í›„ step ìˆ˜ì˜ ì œê³±ê·¼ì— ë°˜ë¹„ë¡€í•˜ì—¬ ê°ì†Œ

**ì™œ Warmupì´ í•„ìš”í•œê°€?**
1. **íŒŒë¼ë¯¸í„° ì´ˆê¸°í™” ë¬¸ì œ**: ì´ˆê¸°ì— íŒŒë¼ë¯¸í„°ê°€ ë¶ˆì•ˆì •í•œ ìƒíƒœ
2. **í° learning rate ìœ„í—˜**: ì´ˆê¸° í° LRì€ ë°œì‚° ìœ„í—˜
3. **ì ì§„ì  í•™ìŠµ**: ì‘ì€ LRë¡œ ì‹œì‘í•´ ì•ˆì •í™” í›„ ë³¸ê²© í•™ìŠµ

**ì‹œê°í™”**:
```
Learning Rate
â”‚
â”‚     /â•²
â”‚    /  â•²___
â”‚   /       â•²___
â”‚  /            â•²___
â”‚ /                 â•²___
â”‚/________________________
  warmup    decay phase
  (4000)
```

**êµ¬í˜„ ì˜ˆì‹œ**:
```python
class NoamOptimizer:
    def __init__(self, d_model, warmup_steps, optimizer):
        self.d_model = d_model
        self.warmup_steps = warmup_steps
        self.optimizer = optimizer
        self.step_num = 0

    def step(self):
        self.step_num += 1
        lr = self.d_model ** (-0.5) * min(
            self.step_num ** (-0.5),
            self.step_num * self.warmup_steps ** (-1.5)
        )
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
        self.optimizer.step()
```

**Regularization**:
1. **Dropout (P_drop = 0.1)**:
   - ê° Sub-layer ì¶œë ¥ì— ì ìš©
   - Attention weightsì—ë„ ì ìš©
   - Embeddings + Positional Encodingì—ë„ ì ìš©

2. **Label Smoothing (Îµ = 0.1)**:
   ```python
   # Hard label: [0, 0, 1, 0, 0]
   # Smoothed label: [0.025, 0.025, 0.9, 0.025, 0.025]
   ```
   - **íš¨ê³¼**: Overfitting ë°©ì§€, ëª¨ë¸ì´ ë„ˆë¬´ í™•ì‹ í•˜ì§€ ì•Šë„ë¡
   - **BLEU í–¥ìƒ**: ì •í™•ë„ëŠ” ì•½ê°„ ë–¨ì–´ì§€ì§€ë§Œ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ## ğŸ’» ì‹¤ì‚¬ìš© ì˜ˆì‹œ

### 1. PyTorch ê¸°ë³¸ êµ¬í˜„

```python
import torch
import torch.nn as nn
import math

class ScaledDotProductAttention(nn.Module):
    def __init__(self, d_k):
        super().__init__()
        self.d_k = d_k
        
    def forward(self, Q, K, V, mask=None):
        # Q, K, V: [batch_size, n_heads, seq_len, d_k]
        
        # Attention Score ê³„ì‚°
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        
        # Masking (ì˜µì…˜)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # Softmax
        attn_weights = torch.softmax(scores, dim=-1)
        
        # Valueì™€ ê³±í•˜ê¸°
        output = torch.matmul(attn_weights, V)
        
        return output, attn_weights

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model=512, n_heads=8):
        super().__init__()
        assert d_model % n_heads == 0
        
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        # Linear layers for Q, K, V
        self.W_Q = nn.Linear(d_model, d_model)
        self.W_K = nn.Linear(d_model, d_model)
        self.W_V = nn.Linear(d_model, d_model)
        self.W_O = nn.Linear(d_model, d_model)
        
        self.attention = ScaledDotProductAttention(self.d_k)
        
    def forward(self, Q, K, V, mask=None):
        batch_size = Q.size(0)
        
        # Linear projection and split into heads
        Q = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        K = self.W_K(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        V = self.W_V(V).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        
        # Attention
        x, attn_weights = self.attention(Q, K, V, mask)
        
        # Concatenate heads
        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        
        # Final linear layer
        output = self.W_O(x)
        
        return output, attn_weights
```### 2. Positional Encoding êµ¬í˜„

```python
import numpy as np

def get_positional_encoding(seq_len, d_model):
    """
    Args:
        seq_len: ì‹œí€€ìŠ¤ ê¸¸ì´
        d_model: ëª¨ë¸ ì°¨ì›
    Returns:
        positional_encoding: [seq_len, d_model]
    """
    position = np.arange(seq_len)[:, np.newaxis]
    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))
    
    pos_encoding = np.zeros((seq_len, d_model))
    pos_encoding[:, 0::2] = np.sin(position * div_term)
    pos_encoding[:, 1::2] = np.cos(position * div_term)
    
    return torch.FloatTensor(pos_encoding)

# ì‚¬ìš© ì˜ˆì‹œ
seq_len = 100
d_model = 512
pos_enc = get_positional_encoding(seq_len, d_model)

# Input embeddingì— ë”í•˜ê¸°
input_embeddings = torch.randn(1, seq_len, d_model)  # [batch, seq_len, d_model]
output = input_embeddings + pos_enc.unsqueeze(0)
```

### 3. Hugging Face Transformers í™œìš©

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# ëª¨ë¸ ë¡œë“œ (ì˜ˆ: T5 - Transformer ê¸°ë°˜)
model_name = "t5-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# ë²ˆì—­ ì˜ˆì‹œ
def translate(text, model, tokenizer):
    # Tokenization
    inputs = tokenizer(
        f"translate English to German: {text}",
        return_tensors="pt",
        max_length=512,
        truncation=True
    )
    
    # Generate
    outputs = model.generate(
        inputs.input_ids,
        max_length=512,
        num_beams=4,
        early_stopping=True
    )
    
    # Decode
    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return translation

# ì‹¤í–‰
text = "Hello, how are you?"
result = translate(text, model, tokenizer)
print(result)
```## ğŸ¯ ì‹¤ë¬´ ì ìš© ì‹œ ê³ ë ¤ì‚¬í•­

### 1. ë©”ëª¨ë¦¬ ìµœì í™”

```python
# Gradient Checkpointing (ë©”ëª¨ë¦¬ ì ˆì•½)
model.gradient_checkpointing_enable()

# Mixed Precision Training
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for batch in dataloader:
    with autocast():
        outputs = model(**batch)
        loss = outputs.loss
    
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

### 2. ê¸´ ì‹œí€€ìŠ¤ ì²˜ë¦¬

```python
# Sliding Window Attention (ê¸´ ë¬¸ì„œ ì²˜ë¦¬)
from transformers import LongformerModel

model = LongformerModel.from_pretrained("allenai/longformer-base-4096")
# ìµœëŒ€ 4096 í† í°ê¹Œì§€ ì²˜ë¦¬ ê°€ëŠ¥
```

### 3. ì¶”ë¡  ìµœì í™”

```python
# Model Quantization (ì¶”ë¡  ì†ë„ í–¥ìƒ)
from transformers import AutoModelForSeq2SeqLM
import torch

model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")
model = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)

# ONNX Export (í”„ë¡œë•ì…˜ ë°°í¬)
from transformers.onnx import export

export(
    preprocessor=tokenizer,
    model=model,
    config=model.config,
    opset=13,
    output=Path("model.onnx")
)
```

## ğŸ§ª Ablation Study (ì†Œê±° ì‹¤í—˜)

ë…¼ë¬¸ì—ì„œëŠ” ê° ì»´í¬ë„ŒíŠ¸ì˜ ì¤‘ìš”ì„±ì„ ê²€ì¦í•˜ê¸° ìœ„í•´ ì²´ê³„ì ì¸ ì‹¤í—˜ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.

### 1. Attention Head ê°œìˆ˜ ë³€í™”

| Heads (h) | d_k | BLEU (EN-DE) | PPL (EN-DE) | Params |
|-----------|-----|--------------|-------------|--------|
| 1 | 512 | 25.8 | 5.29 | 65M |
| 4 | 128 | 27.2 | 4.91 | 65M |
| **8** | **64** | **27.3** | **4.88** | **65M** |
| 16 | 32 | 27.3 | 4.91 | 65M |
| 32 | 16 | 26.5 | 5.01 | 65M |

**ê²°ë¡ **:
- h=8ì´ ìµœì  (ë„ˆë¬´ ë§ê±°ë‚˜ ì ìœ¼ë©´ ì„±ëŠ¥ ì €í•˜)
- ë‹¨ì¼ Headë³´ë‹¤ Multi-Headê°€ í™•ì‹¤íˆ ìš°ìˆ˜ (25.8 vs 27.3)
- d_kê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ (h=32, d_k=16) í‘œí˜„ë ¥ ë¶€ì¡±

### 2. Key/Value ì°¨ì› (d_k) ë³€í™”

| d_k | d_model | BLEU | ë¶„ì„ |
|-----|---------|------|------|
| 64 | 512 | **27.3** | ìµœì  ê· í˜• |
| 128 | 512 | 27.2 | ì•½ê°„ ê³¼ë‹¤ |
| 32 | 512 | 26.4 | í‘œí˜„ë ¥ ë¶€ì¡± |

**í•´ì„**: d_k=64ê°€ ì¶©ë¶„í•œ í‘œí˜„ë ¥ê³¼ ê³„ì‚° íš¨ìœ¨ì˜ ê· í˜•ì 

### 3. Model Size ë¹„êµ

| êµ¬ë¶„ | N | d_model | d_ff | h | Params | BLEU | Train Time |
|------|---|---------|------|---|--------|------|------------|
| Base | 6 | 512 | 2048 | 8 | 65M | 27.3 | 12h |
| Big | 6 | 1024 | 4096 | 16 | 213M | **28.4** | 3.5ì¼ |
| Small | 6 | 256 | 1024 | 4 | 16M | 24.9 | 6h |

### 4. Positional Encoding ë°©ì‹ ë¹„êµ

| ë°©ì‹ | BLEU | ì„¤ëª… |
|------|------|------|
| Sinusoidal (ë…¼ë¬¸) | **27.3** | sin/cos í•¨ìˆ˜ ì‚¬ìš© |
| Learned | 27.2 | í•™ìŠµ ê°€ëŠ¥í•œ embedding |

**ë†€ë¼ìš´ ë°œê²¬**:
- í•™ìŠµëœ positional encodingê³¼ ì„±ëŠ¥ ì°¨ì´ ê±°ì˜ ì—†ìŒ
- Sinusoidalì˜ ì¥ì : í•™ìŠµ ì‹œí€€ìŠ¤ë³´ë‹¤ ê¸´ ì…ë ¥ì—ë„ ì¼ë°˜í™” ê°€ëŠ¥

### 5. Dropout ë¹„ìœ¨ ì˜í–¥

| P_drop | BLEU (Base) | BLEU (Big) |
|--------|-------------|------------|
| 0.0 | 26.8 | 27.6 |
| 0.1 | **27.3** | 28.1 |
| 0.2 | 27.1 | **28.4** |
| 0.3 | 26.9 | 28.3 |

**íŒ¨í„´**: í° ëª¨ë¸ì¼ìˆ˜ë¡ ë” ë†’ì€ dropout í•„ìš” (overfitting ë°©ì§€)

### 6. Attention Type ë¹„êµ

| Attention ì¢…ë¥˜ | EN-DE BLEU | ì„¤ëª… |
|----------------|------------|------|
| Multi-Head (ë…¼ë¬¸) | **27.3** | 8ê°œ Head ë³‘ë ¬ |
| Single-Head | 25.8 | 1ê°œ Headë§Œ |
| Multi-Head (no residual) | 24.2 | Residual ì œê±° ì‹œ |
| Multi-Head (no LayerNorm) | Diverge | í•™ìŠµ ì‹¤íŒ¨ |

**í•µì‹¬ ë°œê²¬**:
- **Multi-Head í•„ìˆ˜**: +1.5 BLEU í–¥ìƒ
- **Residual Connection í•„ìˆ˜**: ì—†ìœ¼ë©´ -3.1 BLEU
- **Layer Normalization í•„ìˆ˜**: ì—†ìœ¼ë©´ í•™ìŠµ ìì²´ê°€ ë¶ˆì•ˆì •

### 7. FFN ì¤‘ê°„ ì°¨ì› (d_ff) ì˜í–¥

| d_ff | d_model | BLEU | ë¶„ì„ |
|------|---------|------|------|
| 1024 | 512 | 26.1 | ìš©ëŸ‰ ë¶€ì¡± |
| **2048** | **512** | **27.3** | ìµœì  (4ë°°) |
| 4096 | 512 | 27.4 | ì•½ê°„ í–¥ìƒ (ê³„ì‚° ë¹„ìš© 2ë°°) |

**ê²½í—˜ì  ë²•ì¹™**: d_ff = 4 Ã— d_modelì´ íš¨ìœ¨ì 

## ğŸ”¬ Attention ì‹œê°í™”

### Attention Weights í™•ì¸

```python
import matplotlib.pyplot as plt
import seaborn as sns

def visualize_attention(model, tokenizer, text):
    inputs = tokenizer(text, return_tensors="pt")
    outputs = model(**inputs, output_attentions=True)
    
    # ì²« ë²ˆì§¸ ë ˆì´ì–´, ì²« ë²ˆì§¸ í—¤ë“œì˜ Attention
    attention = outputs.attentions[0][0, 0].detach().numpy()
    
    tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[0])
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(
        attention,
        xticklabels=tokens,
        yticklabels=tokens,
        cmap="viridis"
    )
    plt.title("Attention Weights")
    plt.show()

# ì‚¬ìš©
text = "The cat sat on the mat"
visualize_attention(model, tokenizer, text)
```

## ğŸ“š ì£¼ìš” ê°œë… ì •ë¦¬

### Q, K, Vì˜ ì§ê´€ì  ì´í•´

**ë”•ì…”ë„ˆë¦¬ ë¹„ìœ **:
```python
# ì¼ë°˜ ë”•ì…”ë„ˆë¦¬
dictionary = {
    "cat": "ê³ ì–‘ì´",
    "dog": "ê°•ì•„ì§€"
}
result = dictionary["cat"]  # ì •í™•íˆ ì¼ì¹˜í•´ì•¼ ê°’ ë°˜í™˜

# Attention Mechanism
# Query: "cat" (ì°¾ê³ ì í•˜ëŠ” ê²ƒ)
# Keys: ["cat", "dog", "animal", ...]
# Values: ["ê³ ì–‘ì´", "ê°•ì•„ì§€", "ë™ë¬¼", ...]
# â†’ "cat"ê³¼ ê° Keyì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ Valueë“¤ì˜ ê°€ì¤‘í•© ë°˜í™˜
```

### Self-Attention vs Cross-Attention

| êµ¬ë¶„ | Self-Attention | Cross-Attention |
|------|----------------|-----------------|
| Q ì¶œì²˜ | ê°™ì€ ì‹œí€€ìŠ¤ | Decoder |
| K, V ì¶œì²˜ | ê°™ì€ ì‹œí€€ìŠ¤ | Encoder |
| ëª©ì  | ë¬¸ì¥ ë‚´ ë‹¨ì–´ ê°„ ê´€ê³„ | ì…ë ¥-ì¶œë ¥ ê°„ ê´€ê³„ |
| ì˜ˆì‹œ | Encoder Self-Attention | Encoder-Decoder Attention |## âš ï¸ ë…¼ë¬¸ì˜ í•œê³„ì 

### 1. ê³„ì‚° ë³µì¡ë„ ë¬¸ì œ
- **O(nÂ²) ë³µì¡ë„**: ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ ë©”ëª¨ë¦¬/ê³„ì‚°ëŸ‰ í­ì¦
  ```
  seq_len = 512  â†’ Attention Matrix: 512Ã—512 = 262K
  seq_len = 2048 â†’ Attention Matrix: 2048Ã—2048 = 4.2M (16ë°° ì¦ê°€)
  ```
- **ê¸´ ë¬¸ì„œ ì²˜ë¦¬ ì–´ë ¤ì›€**: ë…¼ë¬¸ ì‹¤í—˜ì€ ëŒ€ë¶€ë¶„ 100ë‹¨ì–´ ì´í•˜ ë¬¸ì¥
- **ì‹¤ì‹œê°„ ì²˜ë¦¬ í•œê³„**: RNNë³´ë‹¤ ì¶”ë¡  latencyê°€ ë†’ì„ ìˆ˜ ìˆìŒ

### 2. Positional Encodingì˜ í•œê³„
- **ì ˆëŒ€ ìœ„ì¹˜ ì •ë³´**: ìƒëŒ€ì  ìœ„ì¹˜ ê´€ê³„ë¥¼ ì§ì ‘ í•™ìŠµí•˜ì§€ ëª»í•¨
- **ìµœëŒ€ ê¸¸ì´ ì œí•œ**: í•™ìŠµ ì‹œí€€ìŠ¤ë³´ë‹¤ í›¨ì”¬ ê¸´ ì…ë ¥ì€ ì„±ëŠ¥ ì €í•˜
- **í›„ì† ì—°êµ¬ì—ì„œ ê°œì„ **:
  - Relative Positional Encoding (Shaw et al., 2018)
  - Rotary Position Embedding (RoPE, Su et al., 2021)

### 3. Inductive Bias ë¶€ì¡±
- **RNN**: ìˆœì°¨ì„± (sequential bias)
- **CNN**: ì§€ì—­ì„± (locality bias)
- **Transformer**: êµ¬ì¡°ì  ê°€ì • ì—†ìŒ â†’ **ë°ì´í„°ê°€ ë§ì´ í•„ìš”**
  - ì‘ì€ ë°ì´í„°ì…‹ì—ì„œëŠ” RNN/CNNë³´ë‹¤ ì„±ëŠ¥ì´ ë‚®ì„ ìˆ˜ ìˆìŒ

### 4. í•´ì„ ê°€ëŠ¥ì„± ë¬¸ì œ
- Attention weightsê°€ í•­ìƒ ì˜ë¯¸ ìˆëŠ” ê²ƒì€ ì•„ë‹˜
- Multi-Headì˜ ê° Headê°€ ì •í™•íˆ ë¬´ì—‡ì„ í•™ìŠµí•˜ëŠ”ì§€ ë¶ˆëª…í™•
- "Attention is not Explanation" (Jain & Wallace, 2019) ë…¼ìŸ

### 5. ì‹¤ë¬´ì  ì œì•½
- **ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰**: Base model í•™ìŠµì—ë„ 8Ã—P100 GPU í•„ìš”
- **í•™ìŠµ ì‹œê°„**: Big modelì€ 3.5ì¼ (ë¹„ìš© ë†’ìŒ)
- **ì—ë„ˆì§€ ì†Œë¹„**: í™˜ê²½ì  ì˜í–¥ (íƒ„ì†Œ ë°œìêµ­)

## ğŸš€ ë°œì „ ë°©í–¥ ë° í›„ì† ì—°êµ¬

### Transformer ì´í›„ ë“±ì¥í•œ ì£¼ìš” ëª¨ë¸

**1. Encoder-only ëª¨ë¸**
- **BERT (2018)**:
  - ì–‘ë°©í–¥ í•™ìŠµìœ¼ë¡œ ë¬¸ë§¥ ì´í•´ í–¥ìƒ
  - Masked Language Modeling (MLM)
  - 11ê°œ NLP íƒœìŠ¤í¬ì—ì„œ SOTA ë‹¬ì„±
- **RoBERTa, ALBERT, ELECTRA**: BERT ê°œì„  ë³€í˜•ë“¤

**2. Decoder-only ëª¨ë¸ (í˜„ëŒ€ LLMì˜ ì£¼ë¥˜)**
- **GPT (2018)**: ìê¸°íšŒê·€ ìƒì„±
- **GPT-2 (2019)**: 1.5B íŒŒë¼ë¯¸í„°, Zero-shot í•™ìŠµ
- **GPT-3 (2020)**: 175B íŒŒë¼ë¯¸í„°, Few-shot í•™ìŠµ
- **GPT-4 (2023)**: ë©€í‹°ëª¨ë‹¬, ì¶”ë¡  ëŠ¥ë ¥ í–¥ìƒ
- **LLaMA, Mistral, Claude**: ì˜¤í”ˆì†ŒìŠ¤/ìƒìš© LLM

**3. Encoder-Decoder ëª¨ë¸**
- **T5 (2019)**: ëª¨ë“  íƒœìŠ¤í¬ë¥¼ Text-to-Textë¡œ í†µí•©
- **BART (2020)**: Denoising autoencoder
- **mT5, mBART**: ë‹¤êµ­ì–´ ë³€í˜•

**4. íš¨ìœ¨ì„± ê°œì„  (O(nÂ²) ë¬¸ì œ í•´ê²°)**
- **Longformer (2020)**: Sliding Window Attention â†’ O(n)
- **BigBird (2020)**: Sparse Attention â†’ O(n)
- **Linformer (2020)**: Low-rank Approximation â†’ O(n)
- **Flash Attention (2022)**: ë©”ëª¨ë¦¬ ìµœì í™” ì•Œê³ ë¦¬ì¦˜

**5. Vision/Multimodal í™•ì¥**
- **Vision Transformer (ViT, 2020)**: ì´ë¯¸ì§€ ë¶„ë¥˜
- **CLIP (2021)**: ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ í•™ìŠµ
- **Flamingo, GPT-4V**: ë©€í‹°ëª¨ë‹¬ LLM

**6. ì•„í‚¤í…ì²˜ ê°œì„ **
- **Sparse Transformers**: ê³„ì‚°ëŸ‰ ê°ì†Œ
- **Mixture of Experts (MoE)**: ì¡°ê±´ë¶€ ê³„ì‚°
- **State Space Models (Mamba, 2023)**: Transformer ëŒ€ì•ˆ

### í•µì‹¬ íŠ¸ë Œë“œ
1. **ìŠ¤ì¼€ì¼ ì—…**: ìˆ˜ì¡° ê°œ íŒŒë¼ë¯¸í„° (GPT-4, PaLM)
2. **íš¨ìœ¨í™”**: Attention ë³µì¡ë„ ì¤„ì´ê¸°
3. **ë©€í‹°ëª¨ë‹¬**: í…ìŠ¤íŠ¸ ë„˜ì–´ ì´ë¯¸ì§€/ì˜¤ë””ì˜¤/ë¹„ë””ì˜¤
4. **In-context Learning**: íŒŒì¸íŠœë‹ ì—†ì´ Few-shotìœ¼ë¡œ í•™ìŠµ

## ğŸ“ í•µì‹¬ êµí›ˆ

1. **ë³‘ë ¬í™”ê°€ í•µì‹¬**: RNNì˜ ìˆœì°¨ ì²˜ë¦¬ ì œì•½ì„ ì œê±°
2. **Attentionì´ ì¶©ë¶„í•˜ë‹¤**: ë³µì¡í•œ êµ¬ì¡° ì—†ì´ë„ SOTA ë‹¬ì„±
3. **Position ì •ë³´ í•„ìˆ˜**: ìˆœì„œ ì •ë³´ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì£¼ì…
4. **Multi-Headì˜ í˜**: ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ë¬¸ë§¥ íŒŒì•…
5. **Scalingì˜ ì¤‘ìš”ì„±**: ëª¨ë¸ í¬ê¸° í™•ì¥ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ

## ğŸ“– ì°¸ê³  ìë£Œ

- **ì› ë…¼ë¬¸**: https://arxiv.org/abs/1706.03762
- **ê³µì‹ ì½”ë“œ**: https://github.com/tensorflow/tensor2tensor
- **The Illustrated Transformer**: https://jalammar.github.io/illustrated-transformer/
- **Annotated Transformer**: http://nlp.seas.harvard.edu/annotated-transformer/

---

**ì´ ë…¼ë¬¸ì€ í˜„ëŒ€ LLMì˜ ê·¼ê°„ì´ ë˜ëŠ” ì•„í‚¤í…ì²˜ë¥¼ ì œì‹œí–ˆìœ¼ë©°, ì‹¤ë¬´ì—ì„œ ë°˜ë“œì‹œ ì´í•´í•´ì•¼ í•  í•„ìˆ˜ ì§€ì‹ì…ë‹ˆë‹¤.**